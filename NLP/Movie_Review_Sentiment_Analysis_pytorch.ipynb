{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f13c54-111c-443e-84a4-03a9b0d382dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf9e068-2813-4012-b820-05ed172ab8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    \"./train.tsv.zip\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa5069c-fdc1-483e-8ab3-f364a7aec496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89174bc5-c93c-4398-a5a0-aa6df5bacbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\n",
    "    \"./test.tsv.zip\",\n",
    "    sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2899cea-9d85-4871-a237-e80e98e4f16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d72ab9f-937c-4268-88a4-9db18ac47dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd0a60a6-79a5-4fbe-bb76-cb6f7cd4a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   PhraseId    156060 non-null  int64 \n",
      " 1   SentenceId  156060 non-null  int64 \n",
      " 2   Phrase      156060 non-null  object\n",
      " 3   Sentiment   156060 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b74625-ac8e-48d4-88b3-517dd54e0544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec4022f-4cbe-45ac-9fd2-0be47e902717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "876eb59b-1c16-4de9-975f-56361b7a15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_single_text(text):\n",
    "    \"\"\"对单个清洗后的文本进行分词\"\"\"\n",
    "    if text == \"\":  # 处理空文本\n",
    "        return []\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7982a47-e1c4-40ef-96b0-dc38a9c8d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 先清洗文本列\n",
    "train_df['Phrase_clean'] = train_df['Phrase'].apply(clean_text)\n",
    "# 2. 对清洗后的文本列批量分词\n",
    "train_df['Phrase_tokens'] = train_df['Phrase_clean'].apply(tokenize_single_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0742be1f-6bae-4c76-b53f-580ac89762a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1行原始文本：A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\n",
      "第1行分词结果：['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose', 'is', 'also', 'good', 'for', 'the', 'gander', 'some', 'of', 'which', 'occasionally', 'amuses', 'but', 'none', 'of', 'which', 'amounts', 'to', 'much', 'of', 'a', 'story']\n",
      "\n",
      "第2行原始文本：A series of escapades demonstrating the adage that what is good for the goose\n",
      "第2行分词结果：['a', 'series', 'of', 'escapades', 'demonstrating', 'the', 'adage', 'that', 'what', 'is', 'good', 'for', 'the', 'goose']\n",
      "\n",
      "第3行原始文本：A series\n",
      "第3行分词结果：['a', 'series']\n",
      "\n",
      "第4行原始文本：A\n",
      "第4行分词结果：['a']\n",
      "\n",
      "第5行原始文本：series\n",
      "第5行分词结果：['series']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    print(f\"第{idx+1}行原始文本：{train_df.loc[idx, 'Phrase']}\")\n",
    "    print(f\"第{idx+1}行分词结果：{train_df.loc[idx, 'Phrase_tokens']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be2eea8e-da1d-451a-a8ad-81a6e7e97c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa91d4e7-0432-4f3a-98be-1cd7a830ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff557d3f-790f-4fb8-8c73-3d742d7db5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_clean</th>\n",
       "      <th>Phrase_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>[a, series, of, escapades, demonstrating, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>[a, series, of, escapades, demonstrating, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>a series</td>\n",
       "      <td>[a, series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>[a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "      <td>hearst s</td>\n",
       "      <td>[hearst, s]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>[forced, avuncular, chortles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>[avuncular, chortles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>[avuncular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>chortles</td>\n",
       "      <td>[chortles]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "0              1           1   \n",
       "1              2           1   \n",
       "2              3           1   \n",
       "3              4           1   \n",
       "4              5           1   \n",
       "...          ...         ...   \n",
       "156055    156056        8544   \n",
       "156056    156057        8544   \n",
       "156057    156058        8544   \n",
       "156058    156059        8544   \n",
       "156059    156060        8544   \n",
       "\n",
       "                                                   Phrase  Sentiment  \\\n",
       "0       A series of escapades demonstrating the adage ...          1   \n",
       "1       A series of escapades demonstrating the adage ...          2   \n",
       "2                                                A series          2   \n",
       "3                                                       A          2   \n",
       "4                                                  series          2   \n",
       "...                                                   ...        ...   \n",
       "156055                                          Hearst 's          2   \n",
       "156056                          forced avuncular chortles          1   \n",
       "156057                                 avuncular chortles          3   \n",
       "156058                                          avuncular          2   \n",
       "156059                                           chortles          2   \n",
       "\n",
       "                                             Phrase_clean  \\\n",
       "0       a series of escapades demonstrating the adage ...   \n",
       "1       a series of escapades demonstrating the adage ...   \n",
       "2                                                a series   \n",
       "3                                                       a   \n",
       "4                                                  series   \n",
       "...                                                   ...   \n",
       "156055                                           hearst s   \n",
       "156056                          forced avuncular chortles   \n",
       "156057                                 avuncular chortles   \n",
       "156058                                          avuncular   \n",
       "156059                                           chortles   \n",
       "\n",
       "                                            Phrase_tokens  \n",
       "0       [a, series, of, escapades, demonstrating, the,...  \n",
       "1       [a, series, of, escapades, demonstrating, the,...  \n",
       "2                                             [a, series]  \n",
       "3                                                     [a]  \n",
       "4                                                [series]  \n",
       "...                                                   ...  \n",
       "156055                                        [hearst, s]  \n",
       "156056                      [forced, avuncular, chortles]  \n",
       "156057                              [avuncular, chortles]  \n",
       "156058                                        [avuncular]  \n",
       "156059                                         [chortles]  \n",
       "\n",
       "[156060 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5255d1a-f19d-47c6-9088-4c99dd34c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_tokens(tokens):\n",
    "    filter_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filter_tokens.append(token)\n",
    "    return filter_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db9e42dc-2609-49cc-a218-1217e79cda04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Phrase_tokens_filtered'] = train_df['Phrase_tokens'].apply(stopwords_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbc28d2b-f8ec-4fd0-9c78-a5370ae25b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_clean</th>\n",
       "      <th>Phrase_tokens</th>\n",
       "      <th>Phrase_tokens_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>[a, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[series, escapades, demonstrating, adage, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>[a, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[series, escapades, demonstrating, adage, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>a series</td>\n",
       "      <td>[a, series]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>156056</td>\n",
       "      <td>8544</td>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "      <td>hearst s</td>\n",
       "      <td>[hearst, s]</td>\n",
       "      <td>[hearst]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>156057</td>\n",
       "      <td>8544</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>[forced, avuncular, chortles]</td>\n",
       "      <td>[forced, avuncular, chortles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>156058</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>[avuncular, chortles]</td>\n",
       "      <td>[avuncular, chortles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>156059</td>\n",
       "      <td>8544</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>[avuncular]</td>\n",
       "      <td>[avuncular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>156060</td>\n",
       "      <td>8544</td>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>chortles</td>\n",
       "      <td>[chortles]</td>\n",
       "      <td>[chortles]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhraseId  SentenceId  \\\n",
       "0              1           1   \n",
       "1              2           1   \n",
       "2              3           1   \n",
       "3              4           1   \n",
       "4              5           1   \n",
       "...          ...         ...   \n",
       "156055    156056        8544   \n",
       "156056    156057        8544   \n",
       "156057    156058        8544   \n",
       "156058    156059        8544   \n",
       "156059    156060        8544   \n",
       "\n",
       "                                                   Phrase  Sentiment  \\\n",
       "0       A series of escapades demonstrating the adage ...          1   \n",
       "1       A series of escapades demonstrating the adage ...          2   \n",
       "2                                                A series          2   \n",
       "3                                                       A          2   \n",
       "4                                                  series          2   \n",
       "...                                                   ...        ...   \n",
       "156055                                          Hearst 's          2   \n",
       "156056                          forced avuncular chortles          1   \n",
       "156057                                 avuncular chortles          3   \n",
       "156058                                          avuncular          2   \n",
       "156059                                           chortles          2   \n",
       "\n",
       "                                             Phrase_clean  \\\n",
       "0       a series of escapades demonstrating the adage ...   \n",
       "1       a series of escapades demonstrating the adage ...   \n",
       "2                                                a series   \n",
       "3                                                       a   \n",
       "4                                                  series   \n",
       "...                                                   ...   \n",
       "156055                                           hearst s   \n",
       "156056                          forced avuncular chortles   \n",
       "156057                                 avuncular chortles   \n",
       "156058                                          avuncular   \n",
       "156059                                           chortles   \n",
       "\n",
       "                                            Phrase_tokens  \\\n",
       "0       [a, series, of, escapades, demonstrating, the,...   \n",
       "1       [a, series, of, escapades, demonstrating, the,...   \n",
       "2                                             [a, series]   \n",
       "3                                                     [a]   \n",
       "4                                                [series]   \n",
       "...                                                   ...   \n",
       "156055                                        [hearst, s]   \n",
       "156056                      [forced, avuncular, chortles]   \n",
       "156057                              [avuncular, chortles]   \n",
       "156058                                        [avuncular]   \n",
       "156059                                         [chortles]   \n",
       "\n",
       "                                   Phrase_tokens_filtered  \n",
       "0       [series, escapades, demonstrating, adage, good...  \n",
       "1       [series, escapades, demonstrating, adage, good...  \n",
       "2                                                [series]  \n",
       "3                                                      []  \n",
       "4                                                [series]  \n",
       "...                                                   ...  \n",
       "156055                                           [hearst]  \n",
       "156056                      [forced, avuncular, chortles]  \n",
       "156057                              [avuncular, chortles]  \n",
       "156058                                        [avuncular]  \n",
       "156059                                         [chortles]  \n",
       "\n",
       "[156060 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b30fb2-f43a-4d82-ad1b-8e51c538d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['PhraseId','SentenceId'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01e3d801-b6be-40ad-a6c5-5481d54625c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_clean</th>\n",
       "      <th>Phrase_tokens</th>\n",
       "      <th>Phrase_tokens_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>[a, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[series, escapades, demonstrating, adage, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>[a, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[series, escapades, demonstrating, adage, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>a series</td>\n",
       "      <td>[a, series]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[series]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "      <td>hearst s</td>\n",
       "      <td>[hearst, s]</td>\n",
       "      <td>[hearst]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>[forced, avuncular, chortles]</td>\n",
       "      <td>[forced, avuncular, chortles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>[avuncular, chortles]</td>\n",
       "      <td>[avuncular, chortles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>[avuncular]</td>\n",
       "      <td>[avuncular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>chortles</td>\n",
       "      <td>[chortles]</td>\n",
       "      <td>[chortles]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Phrase  Sentiment  \\\n",
       "0       A series of escapades demonstrating the adage ...          1   \n",
       "1       A series of escapades demonstrating the adage ...          2   \n",
       "2                                                A series          2   \n",
       "3                                                       A          2   \n",
       "4                                                  series          2   \n",
       "...                                                   ...        ...   \n",
       "156055                                          Hearst 's          2   \n",
       "156056                          forced avuncular chortles          1   \n",
       "156057                                 avuncular chortles          3   \n",
       "156058                                          avuncular          2   \n",
       "156059                                           chortles          2   \n",
       "\n",
       "                                             Phrase_clean  \\\n",
       "0       a series of escapades demonstrating the adage ...   \n",
       "1       a series of escapades demonstrating the adage ...   \n",
       "2                                                a series   \n",
       "3                                                       a   \n",
       "4                                                  series   \n",
       "...                                                   ...   \n",
       "156055                                           hearst s   \n",
       "156056                          forced avuncular chortles   \n",
       "156057                                 avuncular chortles   \n",
       "156058                                          avuncular   \n",
       "156059                                           chortles   \n",
       "\n",
       "                                            Phrase_tokens  \\\n",
       "0       [a, series, of, escapades, demonstrating, the,...   \n",
       "1       [a, series, of, escapades, demonstrating, the,...   \n",
       "2                                             [a, series]   \n",
       "3                                                     [a]   \n",
       "4                                                [series]   \n",
       "...                                                   ...   \n",
       "156055                                        [hearst, s]   \n",
       "156056                      [forced, avuncular, chortles]   \n",
       "156057                              [avuncular, chortles]   \n",
       "156058                                        [avuncular]   \n",
       "156059                                         [chortles]   \n",
       "\n",
       "                                   Phrase_tokens_filtered  \n",
       "0       [series, escapades, demonstrating, adage, good...  \n",
       "1       [series, escapades, demonstrating, adage, good...  \n",
       "2                                                [series]  \n",
       "3                                                      []  \n",
       "4                                                [series]  \n",
       "...                                                   ...  \n",
       "156055                                           [hearst]  \n",
       "156056                      [forced, avuncular, chortles]  \n",
       "156057                              [avuncular, chortles]  \n",
       "156058                                        [avuncular]  \n",
       "156059                                         [chortles]  \n",
       "\n",
       "[156060 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82318bd5-6206-40d4-80ad-5e67a621c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_for_tfidf'] = train_df['Phrase_tokens_filtered'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e7bef55-d1f6-4754-8edc-4811e082d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4fbfeb9-0b9a-40d3-b4da-0982cdf36d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33cd132d-011c-47c5-af28-0945468f35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['text_for_tfidf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c25b56d-e479-496a-8e4a-00ab0adccbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c9136ac-98be-43b2-a894-5d08ba279cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),  # 稀疏矩阵转密集矩阵\n",
    "    columns=feature_names,   # 列名为词汇\n",
    "    index=train_df.index     # 行索引和原数据一致\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0959168-4d34-4405-b7e5-55df072448f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 矩阵形状：(156060, 5000)\n",
      "提取的特征词汇数量：5000\n",
      "\n",
      "前10个特征词汇： ['10' '100' '101' '11' '12' '12yearold' '13' '15' '18' '18yearold']\n"
     ]
    }
   ],
   "source": [
    "print(f\"TF-IDF 矩阵形状：{tfidf_matrix.shape}\")  # (样本数, 特征数)\n",
    "print(f\"提取的特征词汇数量：{len(feature_names)}\")\n",
    "print(\"\\n前10个特征词汇：\", feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8570c576-1738-41c1-845f-f85f158bfef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4ddf8e2-d478-489a-b3c8-ce84b130a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_df['Phrase_tokens_filtered'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dd5aef1-eb66-41b8-a5ca-e0acbcc4f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [tokens for tokens in corpus if len(tokens) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8292168-2bef-471f-bb71-10c9d71f97de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['series',\n",
       "  'escapades',\n",
       "  'demonstrating',\n",
       "  'adage',\n",
       "  'good',\n",
       "  'goose',\n",
       "  'also',\n",
       "  'good',\n",
       "  'gander',\n",
       "  'occasionally',\n",
       "  'amuses',\n",
       "  'none',\n",
       "  'amounts',\n",
       "  'much',\n",
       "  'story'],\n",
       " ['series', 'escapades', 'demonstrating', 'adage', 'good', 'goose'],\n",
       " ['series'],\n",
       " ['series'],\n",
       " ['escapades', 'demonstrating', 'adage', 'good', 'goose'],\n",
       " ['escapades', 'demonstrating', 'adage', 'good', 'goose'],\n",
       " ['escapades'],\n",
       " ['demonstrating', 'adage', 'good', 'goose'],\n",
       " ['demonstrating', 'adage'],\n",
       " ['demonstrating'],\n",
       " ['adage'],\n",
       " ['adage'],\n",
       " ['good', 'goose'],\n",
       " ['good', 'goose'],\n",
       " ['good', 'goose'],\n",
       " ['good', 'goose'],\n",
       " ['good'],\n",
       " ['goose'],\n",
       " ['goose'],\n",
       " ['goose'],\n",
       " ['also',\n",
       "  'good',\n",
       "  'gander',\n",
       "  'occasionally',\n",
       "  'amuses',\n",
       "  'none',\n",
       "  'amounts',\n",
       "  'much',\n",
       "  'story'],\n",
       " ['also',\n",
       "  'good',\n",
       "  'gander',\n",
       "  'occasionally',\n",
       "  'amuses',\n",
       "  'none',\n",
       "  'amounts',\n",
       "  'much',\n",
       "  'story'],\n",
       " ['also'],\n",
       " ['also'],\n",
       " ['good',\n",
       "  'gander',\n",
       "  'occasionally',\n",
       "  'amuses',\n",
       "  'none',\n",
       "  'amounts',\n",
       "  'much',\n",
       "  'story'],\n",
       " ['gander', 'occasionally', 'amuses', 'none', 'amounts', 'much', 'story'],\n",
       " ['gander', 'occasionally', 'amuses', 'none', 'amounts', 'much', 'story'],\n",
       " ['gander'],\n",
       " ['gander'],\n",
       " ['gander'],\n",
       " ['occasionally', 'amuses', 'none', 'amounts', 'much', 'story'],\n",
       " ['occasionally', 'amuses', 'none', 'amounts', 'much', 'story'],\n",
       " ['occasionally'],\n",
       " ['amuses', 'none', 'amounts', 'much', 'story'],\n",
       " ['amuses'],\n",
       " ['none', 'amounts', 'much', 'story'],\n",
       " ['none', 'amounts', 'much', 'story'],\n",
       " ['none'],\n",
       " ['amounts', 'much', 'story'],\n",
       " ['amounts', 'much', 'story'],\n",
       " ['amounts', 'much', 'story'],\n",
       " ['amounts'],\n",
       " ['much', 'story'],\n",
       " ['much', 'story'],\n",
       " ['much'],\n",
       " ['story'],\n",
       " ['story'],\n",
       " ['story'],\n",
       " ['quiet', 'introspective', 'entertaining', 'independent', 'worth', 'seeking'],\n",
       " ['quiet', 'introspective', 'entertaining', 'independent'],\n",
       " ['quiet', 'introspective', 'entertaining', 'independent'],\n",
       " ['quiet', 'introspective', 'entertaining'],\n",
       " ['quiet'],\n",
       " ['introspective', 'entertaining'],\n",
       " ['introspective', 'entertaining'],\n",
       " ['introspective'],\n",
       " ['introspective'],\n",
       " ['entertaining'],\n",
       " ['independent'],\n",
       " ['worth', 'seeking'],\n",
       " ['worth', 'seeking'],\n",
       " ['worth'],\n",
       " ['worth'],\n",
       " ['seeking'],\n",
       " ['even',\n",
       "  'fans',\n",
       "  'ismail',\n",
       "  'merchant',\n",
       "  'work',\n",
       "  'suspect',\n",
       "  'would',\n",
       "  'hard',\n",
       "  'time',\n",
       "  'sitting',\n",
       "  'one'],\n",
       " ['even', 'fans', 'ismail', 'merchant', 'work'],\n",
       " ['even', 'fans'],\n",
       " ['even'],\n",
       " ['fans'],\n",
       " ['ismail', 'merchant', 'work'],\n",
       " ['ismail', 'merchant', 'work'],\n",
       " ['ismail', 'merchant'],\n",
       " ['ismail'],\n",
       " ['merchant'],\n",
       " ['merchant'],\n",
       " ['work'],\n",
       " ['suspect', 'would', 'hard', 'time', 'sitting', 'one'],\n",
       " ['suspect'],\n",
       " ['suspect'],\n",
       " ['suspect'],\n",
       " ['suspect'],\n",
       " ['would', 'hard', 'time', 'sitting', 'one'],\n",
       " ['would', 'hard', 'time', 'sitting', 'one'],\n",
       " ['would'],\n",
       " ['hard', 'time', 'sitting', 'one'],\n",
       " ['hard', 'time', 'sitting', 'one'],\n",
       " ['hard', 'time'],\n",
       " ['hard', 'time'],\n",
       " ['hard'],\n",
       " ['time'],\n",
       " ['sitting', 'one'],\n",
       " ['sitting'],\n",
       " ['one'],\n",
       " ['one'],\n",
       " ['one'],\n",
       " ['positively',\n",
       "  'thrilling',\n",
       "  'combination',\n",
       "  'ethnography',\n",
       "  'intrigue',\n",
       "  'betrayal',\n",
       "  'deceit',\n",
       "  'murder',\n",
       "  'shakespearean',\n",
       "  'tragedy',\n",
       "  'juicy',\n",
       "  'soap',\n",
       "  'opera'],\n",
       " ['positively',\n",
       "  'thrilling',\n",
       "  'combination',\n",
       "  'ethnography',\n",
       "  'intrigue',\n",
       "  'betrayal',\n",
       "  'deceit',\n",
       "  'murder',\n",
       "  'shakespearean',\n",
       "  'tragedy',\n",
       "  'juicy',\n",
       "  'soap',\n",
       "  'opera'],\n",
       " ['positively',\n",
       "  'thrilling',\n",
       "  'combination',\n",
       "  'ethnography',\n",
       "  'intrigue',\n",
       "  'betrayal',\n",
       "  'deceit',\n",
       "  'murder'],\n",
       " ['positively', 'thrilling', 'combination'],\n",
       " ['positively', 'thrilling', 'combination'],\n",
       " ['positively'],\n",
       " ['thrilling', 'combination'],\n",
       " ['thrilling'],\n",
       " ['combination'],\n",
       " ['ethnography', 'intrigue', 'betrayal', 'deceit', 'murder'],\n",
       " ['ethnography', 'intrigue', 'betrayal', 'deceit', 'murder'],\n",
       " ['ethnography'],\n",
       " ['ethnography'],\n",
       " ['intrigue', 'betrayal', 'deceit', 'murder'],\n",
       " ['intrigue', 'betrayal', 'deceit', 'murder'],\n",
       " ['intrigue', 'betrayal', 'deceit', 'murder'],\n",
       " ['intrigue'],\n",
       " ['betrayal', 'deceit', 'murder'],\n",
       " ['betrayal', 'deceit', 'murder'],\n",
       " ['betrayal'],\n",
       " ['deceit', 'murder'],\n",
       " ['deceit', 'murder'],\n",
       " ['deceit'],\n",
       " ['deceit'],\n",
       " ['murder'],\n",
       " ['shakespearean', 'tragedy', 'juicy', 'soap', 'opera'],\n",
       " ['shakespearean', 'tragedy', 'juicy', 'soap', 'opera'],\n",
       " ['shakespearean', 'tragedy'],\n",
       " ['shakespearean', 'tragedy'],\n",
       " ['shakespearean', 'tragedy'],\n",
       " ['shakespearean'],\n",
       " ['tragedy'],\n",
       " ['juicy', 'soap', 'opera'],\n",
       " ['juicy', 'soap', 'opera'],\n",
       " ['juicy'],\n",
       " ['soap', 'opera'],\n",
       " ['soap'],\n",
       " ['opera'],\n",
       " ['aggressive', 'selfglorification', 'manipulative', 'whitewash'],\n",
       " ['aggressive', 'selfglorification', 'manipulative', 'whitewash'],\n",
       " ['aggressive'],\n",
       " ['selfglorification', 'manipulative', 'whitewash'],\n",
       " ['selfglorification'],\n",
       " ['selfglorification'],\n",
       " ['manipulative', 'whitewash'],\n",
       " ['manipulative', 'whitewash'],\n",
       " ['manipulative'],\n",
       " ['whitewash'],\n",
       " ['comedydrama',\n",
       "  'nearly',\n",
       "  'epic',\n",
       "  'proportions',\n",
       "  'rooted',\n",
       "  'sincere',\n",
       "  'performance',\n",
       "  'title',\n",
       "  'character',\n",
       "  'undergoing',\n",
       "  'midlife',\n",
       "  'crisis'],\n",
       " ['comedydrama', 'nearly', 'epic', 'proportions'],\n",
       " ['comedydrama'],\n",
       " ['comedydrama'],\n",
       " ['nearly', 'epic', 'proportions'],\n",
       " ['nearly', 'epic', 'proportions'],\n",
       " ['nearly', 'epic'],\n",
       " ['nearly'],\n",
       " ['epic'],\n",
       " ['proportions'],\n",
       " ['rooted',\n",
       "  'sincere',\n",
       "  'performance',\n",
       "  'title',\n",
       "  'character',\n",
       "  'undergoing',\n",
       "  'midlife',\n",
       "  'crisis'],\n",
       " ['rooted',\n",
       "  'sincere',\n",
       "  'performance',\n",
       "  'title',\n",
       "  'character',\n",
       "  'undergoing',\n",
       "  'midlife',\n",
       "  'crisis'],\n",
       " ['rooted', 'sincere', 'performance'],\n",
       " ['rooted'],\n",
       " ['sincere', 'performance'],\n",
       " ['sincere', 'performance'],\n",
       " ['sincere', 'performance'],\n",
       " ['sincere'],\n",
       " ['performance'],\n",
       " ['title', 'character', 'undergoing', 'midlife', 'crisis'],\n",
       " ['title', 'character', 'undergoing', 'midlife', 'crisis'],\n",
       " ['title', 'character'],\n",
       " ['title', 'character'],\n",
       " ['title'],\n",
       " ['character'],\n",
       " ['undergoing', 'midlife', 'crisis'],\n",
       " ['undergoing'],\n",
       " ['midlife', 'crisis'],\n",
       " ['midlife'],\n",
       " ['crisis'],\n",
       " ['narratively', 'trouble', 'every', 'day', 'plodding', 'mess'],\n",
       " ['narratively'],\n",
       " ['trouble', 'every', 'day', 'plodding', 'mess'],\n",
       " ['trouble', 'every', 'day', 'plodding', 'mess'],\n",
       " ['trouble', 'every', 'day'],\n",
       " ['trouble'],\n",
       " ['every', 'day'],\n",
       " ['every'],\n",
       " ['day'],\n",
       " ['plodding', 'mess'],\n",
       " ['plodding', 'mess'],\n",
       " ['plodding', 'mess'],\n",
       " ['plodding', 'mess'],\n",
       " ['plodding'],\n",
       " ['mess'],\n",
       " ['importance',\n",
       "  'earnest',\n",
       "  'thick',\n",
       "  'wit',\n",
       "  'plays',\n",
       "  'like',\n",
       "  'reading',\n",
       "  'bartlett',\n",
       "  'familiar',\n",
       "  'quotations'],\n",
       " ['importance'],\n",
       " ['importance'],\n",
       " ['earnest',\n",
       "  'thick',\n",
       "  'wit',\n",
       "  'plays',\n",
       "  'like',\n",
       "  'reading',\n",
       "  'bartlett',\n",
       "  'familiar',\n",
       "  'quotations'],\n",
       " ['earnest',\n",
       "  'thick',\n",
       "  'wit',\n",
       "  'plays',\n",
       "  'like',\n",
       "  'reading',\n",
       "  'bartlett',\n",
       "  'familiar',\n",
       "  'quotations'],\n",
       " ['earnest',\n",
       "  'thick',\n",
       "  'wit',\n",
       "  'plays',\n",
       "  'like',\n",
       "  'reading',\n",
       "  'bartlett',\n",
       "  'familiar',\n",
       "  'quotations'],\n",
       " ['earnest'],\n",
       " ['earnest'],\n",
       " ['thick',\n",
       "  'wit',\n",
       "  'plays',\n",
       "  'like',\n",
       "  'reading',\n",
       "  'bartlett',\n",
       "  'familiar',\n",
       "  'quotations'],\n",
       " ['thick',\n",
       "  'wit',\n",
       "  'plays',\n",
       "  'like',\n",
       "  'reading',\n",
       "  'bartlett',\n",
       "  'familiar',\n",
       "  'quotations'],\n",
       " ['thick'],\n",
       " ['wit', 'plays', 'like', 'reading', 'bartlett', 'familiar', 'quotations'],\n",
       " ['wit', 'plays', 'like', 'reading', 'bartlett', 'familiar', 'quotations'],\n",
       " ['wit'],\n",
       " ['plays', 'like', 'reading', 'bartlett', 'familiar', 'quotations'],\n",
       " ['plays', 'like', 'reading', 'bartlett', 'familiar', 'quotations'],\n",
       " ['plays'],\n",
       " ['like', 'reading', 'bartlett', 'familiar', 'quotations'],\n",
       " ['like'],\n",
       " ['reading', 'bartlett', 'familiar', 'quotations'],\n",
       " ['reading'],\n",
       " ['reading'],\n",
       " ['bartlett', 'familiar', 'quotations'],\n",
       " ['bartlett', 'familiar', 'quotations'],\n",
       " ['bartlett'],\n",
       " ['bartlett'],\n",
       " ['familiar', 'quotations'],\n",
       " ['familiar'],\n",
       " ['quotations'],\n",
       " ['nt', 'leave', 'much'],\n",
       " ['nt', 'leave', 'much'],\n",
       " ['nt', 'leave', 'much'],\n",
       " ['nt', 'leave', 'much'],\n",
       " ['nt'],\n",
       " ['nt'],\n",
       " ['leave', 'much'],\n",
       " ['leave'],\n",
       " ['leave'],\n",
       " ['much'],\n",
       " ['could', 'hate', 'reason'],\n",
       " ['could', 'hate', 'reason'],\n",
       " ['could', 'hate', 'reason'],\n",
       " ['could'],\n",
       " ['hate', 'reason'],\n",
       " ['hate'],\n",
       " ['hate'],\n",
       " ['reason'],\n",
       " ['reason'],\n",
       " ['reason'],\n",
       " ['reason'],\n",
       " ['little',\n",
       "  'recommend',\n",
       "  'snow',\n",
       "  'dogs',\n",
       "  'unless',\n",
       "  'one',\n",
       "  'considers',\n",
       "  'cliched',\n",
       "  'dialogue',\n",
       "  'perverse',\n",
       "  'escapism',\n",
       "  'source',\n",
       "  'high',\n",
       "  'hilarity'],\n",
       " ['little',\n",
       "  'recommend',\n",
       "  'snow',\n",
       "  'dogs',\n",
       "  'unless',\n",
       "  'one',\n",
       "  'considers',\n",
       "  'cliched',\n",
       "  'dialogue',\n",
       "  'perverse',\n",
       "  'escapism',\n",
       "  'source',\n",
       "  'high',\n",
       "  'hilarity'],\n",
       " ['little',\n",
       "  'recommend',\n",
       "  'snow',\n",
       "  'dogs',\n",
       "  'unless',\n",
       "  'one',\n",
       "  'considers',\n",
       "  'cliched',\n",
       "  'dialogue',\n",
       "  'perverse',\n",
       "  'escapism',\n",
       "  'source',\n",
       "  'high',\n",
       "  'hilarity'],\n",
       " ['little', 'recommend', 'snow', 'dogs'],\n",
       " ['little', 'recommend', 'snow', 'dogs'],\n",
       " ['little', 'recommend', 'snow', 'dogs'],\n",
       " ['little'],\n",
       " ['recommend', 'snow', 'dogs'],\n",
       " ['recommend', 'snow', 'dogs'],\n",
       " ['recommend'],\n",
       " ['snow', 'dogs'],\n",
       " ['snow'],\n",
       " ['dogs'],\n",
       " ['unless',\n",
       "  'one',\n",
       "  'considers',\n",
       "  'cliched',\n",
       "  'dialogue',\n",
       "  'perverse',\n",
       "  'escapism',\n",
       "  'source',\n",
       "  'high',\n",
       "  'hilarity'],\n",
       " ['unless'],\n",
       " ['one',\n",
       "  'considers',\n",
       "  'cliched',\n",
       "  'dialogue',\n",
       "  'perverse',\n",
       "  'escapism',\n",
       "  'source',\n",
       "  'high',\n",
       "  'hilarity'],\n",
       " ['considers',\n",
       "  'cliched',\n",
       "  'dialogue',\n",
       "  'perverse',\n",
       "  'escapism',\n",
       "  'source',\n",
       "  'high',\n",
       "  'hilarity'],\n",
       " ['considers'],\n",
       " ['cliched', 'dialogue', 'perverse', 'escapism', 'source', 'high', 'hilarity'],\n",
       " ['cliched', 'dialogue', 'perverse', 'escapism'],\n",
       " ['cliched', 'dialogue'],\n",
       " ['cliched', 'dialogue'],\n",
       " ['cliched'],\n",
       " ['dialogue'],\n",
       " ['perverse', 'escapism'],\n",
       " ['perverse'],\n",
       " ['escapism'],\n",
       " ['source', 'high', 'hilarity'],\n",
       " ['source'],\n",
       " ['source'],\n",
       " ['high', 'hilarity'],\n",
       " ['high', 'hilarity'],\n",
       " ['high'],\n",
       " ['hilarity'],\n",
       " ['kung',\n",
       "  'pow',\n",
       "  'oedekerk',\n",
       "  'realization',\n",
       "  'childhood',\n",
       "  'dream',\n",
       "  'martialarts',\n",
       "  'flick',\n",
       "  'proves',\n",
       "  'sometimes',\n",
       "  'dreams',\n",
       "  'youth',\n",
       "  'remain'],\n",
       " ['kung', 'pow'],\n",
       " ['kung'],\n",
       " ['pow'],\n",
       " ['oedekerk',\n",
       "  'realization',\n",
       "  'childhood',\n",
       "  'dream',\n",
       "  'martialarts',\n",
       "  'flick',\n",
       "  'proves',\n",
       "  'sometimes',\n",
       "  'dreams',\n",
       "  'youth',\n",
       "  'remain'],\n",
       " ['oedekerk',\n",
       "  'realization',\n",
       "  'childhood',\n",
       "  'dream',\n",
       "  'martialarts',\n",
       "  'flick',\n",
       "  'proves',\n",
       "  'sometimes',\n",
       "  'dreams',\n",
       "  'youth',\n",
       "  'remain'],\n",
       " ['oedekerk', 'realization', 'childhood', 'dream', 'martialarts', 'flick'],\n",
       " ['oedekerk', 'realization', 'childhood', 'dream', 'martialarts', 'flick'],\n",
       " ['oedekerk', 'realization', 'childhood', 'dream', 'martialarts', 'flick'],\n",
       " ['oedekerk', 'realization', 'childhood', 'dream', 'martialarts', 'flick'],\n",
       " ['oedekerk', 'realization', 'childhood', 'dream'],\n",
       " ['oedekerk', 'realization'],\n",
       " ['oedekerk'],\n",
       " ['oedekerk'],\n",
       " ['realization'],\n",
       " ['childhood', 'dream'],\n",
       " ['childhood', 'dream'],\n",
       " ['childhood', 'dream'],\n",
       " ['childhood'],\n",
       " ['dream'],\n",
       " ['martialarts', 'flick'],\n",
       " ['martialarts', 'flick'],\n",
       " ['martialarts', 'flick'],\n",
       " ['martialarts', 'flick'],\n",
       " ['martialarts', 'flick'],\n",
       " ['martialarts'],\n",
       " ['flick'],\n",
       " ['proves', 'sometimes', 'dreams', 'youth', 'remain'],\n",
       " ['proves'],\n",
       " ['sometimes', 'dreams', 'youth', 'remain'],\n",
       " ['sometimes', 'dreams', 'youth', 'remain'],\n",
       " ['sometimes'],\n",
       " ['dreams', 'youth', 'remain'],\n",
       " ['dreams', 'youth'],\n",
       " ['dreams'],\n",
       " ['dreams'],\n",
       " ['youth'],\n",
       " ['youth'],\n",
       " ['remain'],\n",
       " ['remain'],\n",
       " ['remain'],\n",
       " ['performances', 'absolute', 'joy'],\n",
       " ['performances'],\n",
       " ['performances'],\n",
       " ['absolute', 'joy'],\n",
       " ['absolute', 'joy'],\n",
       " ['absolute', 'joy'],\n",
       " ['absolute', 'joy'],\n",
       " ['absolute'],\n",
       " ['joy'],\n",
       " ['fresnadillo',\n",
       "  'something',\n",
       "  'serious',\n",
       "  'say',\n",
       "  'ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['fresnadillo'],\n",
       " ['something',\n",
       "  'serious',\n",
       "  'say',\n",
       "  'ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['something',\n",
       "  'serious',\n",
       "  'say',\n",
       "  'ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['something',\n",
       "  'serious',\n",
       "  'say',\n",
       "  'ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['something'],\n",
       " ['serious',\n",
       "  'say',\n",
       "  'ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['serious'],\n",
       " ['say',\n",
       "  'ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['say',\n",
       "  'ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['say'],\n",
       " ['ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['ways',\n",
       "  'extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['ways'],\n",
       " ['ways'],\n",
       " ['extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['extravagant',\n",
       "  'chance',\n",
       "  'distort',\n",
       "  'perspective',\n",
       "  'throw',\n",
       "  'us',\n",
       "  'path',\n",
       "  'good',\n",
       "  'sense'],\n",
       " ['extravagant', 'chance'],\n",
       " ['extravagant'],\n",
       " ['chance'],\n",
       " ['distort', 'perspective', 'throw', 'us', 'path', 'good', 'sense'],\n",
       " ['distort', 'perspective', 'throw', 'us', 'path', 'good', 'sense'],\n",
       " ['distort', 'perspective'],\n",
       " ['distort', 'perspective'],\n",
       " ['distort'],\n",
       " ['perspective'],\n",
       " ['perspective'],\n",
       " ['throw', 'us', 'path', 'good', 'sense'],\n",
       " ['throw', 'us'],\n",
       " ['throw'],\n",
       " ['us'],\n",
       " ['path', 'good', 'sense'],\n",
       " ['path', 'good', 'sense'],\n",
       " ['path'],\n",
       " ['path'],\n",
       " ['good', 'sense'],\n",
       " ['good', 'sense'],\n",
       " ['sense'],\n",
       " ['still', 'like', 'moonlight', 'mile', 'better', 'judgment', 'damned'],\n",
       " ['still', 'like', 'moonlight', 'mile', 'better', 'judgment', 'damned'],\n",
       " ['still'],\n",
       " ['like', 'moonlight', 'mile', 'better', 'judgment', 'damned'],\n",
       " ['like', 'moonlight', 'mile', 'better', 'judgment', 'damned'],\n",
       " ['moonlight', 'mile', 'better', 'judgment', 'damned'],\n",
       " ['moonlight', 'mile', 'better', 'judgment'],\n",
       " ['moonlight'],\n",
       " ['mile', 'better', 'judgment'],\n",
       " ['mile'],\n",
       " ['better', 'judgment'],\n",
       " ['better', 'judgment'],\n",
       " ['better'],\n",
       " ['judgment'],\n",
       " ['damned'],\n",
       " ['damned'],\n",
       " ['welcome',\n",
       "  'relief',\n",
       "  'baseball',\n",
       "  'movies',\n",
       "  'try',\n",
       "  'hard',\n",
       "  'mythic',\n",
       "  'one',\n",
       "  'sweet',\n",
       "  'modest',\n",
       "  'ultimately',\n",
       "  'winning',\n",
       "  'story'],\n",
       " ['welcome', 'relief', 'baseball', 'movies', 'try', 'hard', 'mythic'],\n",
       " ['welcome', 'relief'],\n",
       " ['welcome', 'relief'],\n",
       " ['welcome'],\n",
       " ['relief'],\n",
       " ['baseball', 'movies', 'try', 'hard', 'mythic'],\n",
       " ['baseball', 'movies', 'try', 'hard', 'mythic'],\n",
       " ['baseball', 'movies'],\n",
       " ['baseball'],\n",
       " ['movies'],\n",
       " ['try', 'hard', 'mythic'],\n",
       " ['try', 'hard', 'mythic'],\n",
       " ['try'],\n",
       " ['hard', 'mythic'],\n",
       " ['hard', 'mythic'],\n",
       " ['mythic'],\n",
       " ['mythic'],\n",
       " ['mythic'],\n",
       " ['one', 'sweet', 'modest', 'ultimately', 'winning', 'story'],\n",
       " ['one', 'sweet', 'modest', 'ultimately', 'winning', 'story'],\n",
       " ['sweet', 'modest', 'ultimately', 'winning', 'story'],\n",
       " ['sweet', 'modest', 'ultimately', 'winning', 'story'],\n",
       " ['sweet', 'modest', 'ultimately', 'winning', 'story'],\n",
       " ['sweet', 'modest'],\n",
       " ['sweet', 'modest'],\n",
       " ['sweet', 'modest'],\n",
       " ['sweet'],\n",
       " ['sweet'],\n",
       " ['modest'],\n",
       " ['ultimately', 'winning', 'story'],\n",
       " ['ultimately'],\n",
       " ['winning', 'story'],\n",
       " ['winning'],\n",
       " ['bilingual', 'charmer', 'like', 'woman', 'inspired'],\n",
       " ['bilingual', 'charmer'],\n",
       " ['bilingual', 'charmer'],\n",
       " ['bilingual', 'charmer'],\n",
       " ['bilingual'],\n",
       " ['charmer'],\n",
       " ['like', 'woman', 'inspired'],\n",
       " ['like', 'woman', 'inspired'],\n",
       " ['woman', 'inspired'],\n",
       " ['woman'],\n",
       " ['woman'],\n",
       " ['inspired'],\n",
       " ['inspired'],\n",
       " ['inspired'],\n",
       " ['like',\n",
       "  'less',\n",
       "  'dizzily',\n",
       "  'gorgeous',\n",
       "  'companion',\n",
       "  'mr',\n",
       "  'wong',\n",
       "  'mood',\n",
       "  'love',\n",
       "  'much',\n",
       "  'hong',\n",
       "  'kong',\n",
       "  'movie',\n",
       "  'despite',\n",
       "  'mainland',\n",
       "  'setting'],\n",
       " ['like', 'less', 'dizzily', 'gorgeous', 'companion', 'mr'],\n",
       " ['less', 'dizzily', 'gorgeous', 'companion', 'mr'],\n",
       " ['less', 'dizzily', 'gorgeous', 'companion'],\n",
       " ['less', 'dizzily', 'gorgeous', 'companion'],\n",
       " ['less', 'dizzily', 'gorgeous'],\n",
       " ['less'],\n",
       " ['dizzily', 'gorgeous'],\n",
       " ['dizzily'],\n",
       " ['gorgeous'],\n",
       " ['companion'],\n",
       " ['mr'],\n",
       " ['mr'],\n",
       " ['wong',\n",
       "  'mood',\n",
       "  'love',\n",
       "  'much',\n",
       "  'hong',\n",
       "  'kong',\n",
       "  'movie',\n",
       "  'despite',\n",
       "  'mainland',\n",
       "  'setting'],\n",
       " ['wong'],\n",
       " ['mood',\n",
       "  'love',\n",
       "  'much',\n",
       "  'hong',\n",
       "  'kong',\n",
       "  'movie',\n",
       "  'despite',\n",
       "  'mainland',\n",
       "  'setting'],\n",
       " ['mood',\n",
       "  'love',\n",
       "  'much',\n",
       "  'hong',\n",
       "  'kong',\n",
       "  'movie',\n",
       "  'despite',\n",
       "  'mainland',\n",
       "  'setting'],\n",
       " ['mood', 'love', 'much', 'hong', 'kong', 'movie'],\n",
       " ['mood', 'love', 'much', 'hong', 'kong', 'movie'],\n",
       " ['mood', 'love', 'much', 'hong', 'kong', 'movie'],\n",
       " ['mood'],\n",
       " ['mood'],\n",
       " ['love', 'much', 'hong', 'kong', 'movie'],\n",
       " ['love', 'much', 'hong', 'kong', 'movie'],\n",
       " ['love', 'much'],\n",
       " ['love'],\n",
       " ['love'],\n",
       " ['much'],\n",
       " ['hong', 'kong', 'movie'],\n",
       " ['hong', 'kong', 'movie'],\n",
       " ['hong'],\n",
       " ['kong', 'movie'],\n",
       " ['kong'],\n",
       " ['movie'],\n",
       " ['despite', 'mainland', 'setting'],\n",
       " ['despite'],\n",
       " ['mainland', 'setting'],\n",
       " ['mainland', 'setting'],\n",
       " ['mainland'],\n",
       " ['setting'],\n",
       " ['inept', 'bigscreen', 'remakes', 'avengers', 'wild', 'wild', 'west'],\n",
       " ['inept', 'bigscreen', 'remakes', 'avengers', 'wild', 'wild', 'west'],\n",
       " ['inept'],\n",
       " ['inept'],\n",
       " ['bigscreen', 'remakes', 'avengers', 'wild', 'wild', 'west'],\n",
       " ['bigscreen', 'remakes', 'avengers', 'wild', 'wild', 'west'],\n",
       " ['bigscreen', 'remakes'],\n",
       " ['bigscreen'],\n",
       " ['remakes'],\n",
       " ['avengers', 'wild', 'wild', 'west'],\n",
       " ['avengers', 'wild', 'wild', 'west'],\n",
       " ['avengers'],\n",
       " ['avengers'],\n",
       " ['avengers'],\n",
       " ['wild', 'wild', 'west'],\n",
       " ['wild', 'wild', 'west'],\n",
       " ['wild'],\n",
       " ['wild', 'west'],\n",
       " ['west'],\n",
       " ['everything', 'expect', 'nothing'],\n",
       " ['everything', 'expect', 'nothing'],\n",
       " ['everything', 'expect', 'nothing'],\n",
       " ['everything', 'expect', 'nothing'],\n",
       " ['everything'],\n",
       " ['expect', 'nothing'],\n",
       " ['expect', 'nothing'],\n",
       " ['expect', 'nothing'],\n",
       " ['expect', 'nothing'],\n",
       " ['expect'],\n",
       " ['expect'],\n",
       " ['nothing'],\n",
       " ['nothing'],\n",
       " ['best', 'indie', 'year', 'far'],\n",
       " ['best'],\n",
       " ['indie', 'year', 'far'],\n",
       " ['indie', 'year', 'far'],\n",
       " ['indie', 'year'],\n",
       " ['indie', 'year'],\n",
       " ['indie'],\n",
       " ['year'],\n",
       " ['year'],\n",
       " ['year'],\n",
       " ['far'],\n",
       " ['far'],\n",
       " ['hatfield',\n",
       "  'hicks',\n",
       "  'make',\n",
       "  'oddest',\n",
       "  'couples',\n",
       "  'sense',\n",
       "  'movie',\n",
       "  'becomes',\n",
       "  'study',\n",
       "  'gambles',\n",
       "  'publishing',\n",
       "  'world',\n",
       "  'offering',\n",
       "  'case',\n",
       "  'study',\n",
       "  'exists',\n",
       "  'apart',\n",
       "  'movie',\n",
       "  'political',\n",
       "  'ramifications'],\n",
       " ['hatfield',\n",
       "  'hicks',\n",
       "  'make',\n",
       "  'oddest',\n",
       "  'couples',\n",
       "  'sense',\n",
       "  'movie',\n",
       "  'becomes',\n",
       "  'study',\n",
       "  'gambles',\n",
       "  'publishing',\n",
       "  'world',\n",
       "  'offering',\n",
       "  'case',\n",
       "  'study',\n",
       "  'exists',\n",
       "  'apart',\n",
       "  'movie',\n",
       "  'political',\n",
       "  'ramifications'],\n",
       " ['hatfield', 'hicks', 'make', 'oddest', 'couples'],\n",
       " ['hatfield', 'hicks', 'make', 'oddest', 'couples'],\n",
       " ['hatfield', 'hicks', 'make', 'oddest', 'couples'],\n",
       " ['hatfield', 'hicks'],\n",
       " ['hatfield'],\n",
       " ['hatfield'],\n",
       " ['hicks'],\n",
       " ['make', 'oddest', 'couples'],\n",
       " ['make'],\n",
       " ['oddest', 'couples'],\n",
       " ['oddest'],\n",
       " ['oddest'],\n",
       " ['couples'],\n",
       " ['couples'],\n",
       " ['sense',\n",
       "  'movie',\n",
       "  'becomes',\n",
       "  'study',\n",
       "  'gambles',\n",
       "  'publishing',\n",
       "  'world',\n",
       "  'offering',\n",
       "  'case',\n",
       "  'study',\n",
       "  'exists',\n",
       "  'apart',\n",
       "  'movie',\n",
       "  'political',\n",
       "  'ramifications'],\n",
       " ['sense'],\n",
       " ['sense'],\n",
       " ['movie',\n",
       "  'becomes',\n",
       "  'study',\n",
       "  'gambles',\n",
       "  'publishing',\n",
       "  'world',\n",
       "  'offering',\n",
       "  'case',\n",
       "  'study',\n",
       "  'exists',\n",
       "  'apart',\n",
       "  'movie',\n",
       "  'political',\n",
       "  'ramifications'],\n",
       " ['movie'],\n",
       " ['becomes',\n",
       "  'study',\n",
       "  'gambles',\n",
       "  'publishing',\n",
       "  'world',\n",
       "  'offering',\n",
       "  'case',\n",
       "  'study',\n",
       "  'exists',\n",
       "  'apart',\n",
       "  'movie',\n",
       "  'political',\n",
       "  'ramifications'],\n",
       " ['becomes', 'study', 'gambles', 'publishing', 'world'],\n",
       " ['becomes', 'study', 'gambles', 'publishing', 'world'],\n",
       " ['becomes'],\n",
       " ['study', 'gambles', 'publishing', 'world'],\n",
       " ['study'],\n",
       " ['study'],\n",
       " ['gambles', 'publishing', 'world'],\n",
       " ['gambles', 'publishing', 'world'],\n",
       " ['gambles'],\n",
       " ['gambles'],\n",
       " ['publishing', 'world'],\n",
       " ['publishing', 'world'],\n",
       " ['publishing', 'world'],\n",
       " ['publishing'],\n",
       " ['world'],\n",
       " ['offering',\n",
       "  'case',\n",
       "  'study',\n",
       "  'exists',\n",
       "  'apart',\n",
       "  'movie',\n",
       "  'political',\n",
       "  'ramifications'],\n",
       " ['offering'],\n",
       " ['case', 'study', 'exists', 'apart', 'movie', 'political', 'ramifications'],\n",
       " ['case', 'study'],\n",
       " ['case', 'study'],\n",
       " ['case'],\n",
       " ['exists', 'apart', 'movie', 'political', 'ramifications'],\n",
       " ['exists', 'apart', 'movie', 'political', 'ramifications'],\n",
       " ['exists', 'apart'],\n",
       " ['exists'],\n",
       " ['apart'],\n",
       " ['movie', 'political', 'ramifications'],\n",
       " ['movie', 'political', 'ramifications'],\n",
       " ['movie', 'political', 'ramifications'],\n",
       " ['movie'],\n",
       " ['movie'],\n",
       " ['political', 'ramifications'],\n",
       " ['political'],\n",
       " ['ramifications'],\n",
       " ['like',\n",
       "  'going',\n",
       "  'house',\n",
       "  'party',\n",
       "  'watching',\n",
       "  'host',\n",
       "  'defend',\n",
       "  'frothing',\n",
       "  'exgirlfriend'],\n",
       " ['like',\n",
       "  'going',\n",
       "  'house',\n",
       "  'party',\n",
       "  'watching',\n",
       "  'host',\n",
       "  'defend',\n",
       "  'frothing',\n",
       "  'exgirlfriend'],\n",
       " ['like',\n",
       "  'going',\n",
       "  'house',\n",
       "  'party',\n",
       "  'watching',\n",
       "  'host',\n",
       "  'defend',\n",
       "  'frothing',\n",
       "  'exgirlfriend'],\n",
       " ['like',\n",
       "  'going',\n",
       "  'house',\n",
       "  'party',\n",
       "  'watching',\n",
       "  'host',\n",
       "  'defend',\n",
       "  'frothing',\n",
       "  'exgirlfriend'],\n",
       " ['going',\n",
       "  'house',\n",
       "  'party',\n",
       "  'watching',\n",
       "  'host',\n",
       "  'defend',\n",
       "  'frothing',\n",
       "  'exgirlfriend'],\n",
       " ['going', 'house', 'party'],\n",
       " ['going', 'house', 'party'],\n",
       " ['going'],\n",
       " ['house', 'party'],\n",
       " ['house', 'party'],\n",
       " ['house', 'party'],\n",
       " ['house'],\n",
       " ['party'],\n",
       " ['watching', 'host', 'defend', 'frothing', 'exgirlfriend'],\n",
       " ['watching'],\n",
       " ['host', 'defend', 'frothing', 'exgirlfriend'],\n",
       " ['host'],\n",
       " ['host'],\n",
       " ['defend', 'frothing', 'exgirlfriend'],\n",
       " ['defend'],\n",
       " ['defend'],\n",
       " ['frothing', 'exgirlfriend'],\n",
       " ['frothing', 'exgirlfriend'],\n",
       " ['frothing', 'exgirlfriend'],\n",
       " ['frothing'],\n",
       " ['exgirlfriend'],\n",
       " ['chuck',\n",
       "  'norris',\n",
       "  'grenade',\n",
       "  'gag',\n",
       "  'occurs',\n",
       "  '7',\n",
       "  'times',\n",
       "  'windtalkers',\n",
       "  'good',\n",
       "  'indication',\n",
       "  'seriousminded',\n",
       "  'film'],\n",
       " ['chuck', 'norris', 'grenade', 'gag'],\n",
       " ['chuck', 'norris', 'grenade', 'gag'],\n",
       " ['chuck', 'norris', 'grenade', 'gag'],\n",
       " ['chuck'],\n",
       " ['norris', 'grenade', 'gag'],\n",
       " ['norris'],\n",
       " ['grenade', 'gag'],\n",
       " ['grenade', 'gag'],\n",
       " ['grenade'],\n",
       " ['gag'],\n",
       " ['gag'],\n",
       " ['occurs',\n",
       "  '7',\n",
       "  'times',\n",
       "  'windtalkers',\n",
       "  'good',\n",
       "  'indication',\n",
       "  'seriousminded',\n",
       "  'film'],\n",
       " ['occurs',\n",
       "  '7',\n",
       "  'times',\n",
       "  'windtalkers',\n",
       "  'good',\n",
       "  'indication',\n",
       "  'seriousminded',\n",
       "  'film'],\n",
       " ['occurs'],\n",
       " ['7', 'times', 'windtalkers', 'good', 'indication', 'seriousminded', 'film'],\n",
       " ['7', 'times', 'windtalkers'],\n",
       " ['7', 'times'],\n",
       " ['7', 'times'],\n",
       " ['7'],\n",
       " ['times'],\n",
       " ['windtalkers'],\n",
       " ['windtalkers'],\n",
       " ['good', 'indication', 'seriousminded', 'film'],\n",
       " ['good', 'indication', 'seriousminded', 'film'],\n",
       " ['good', 'indication'],\n",
       " ['good', 'indication'],\n",
       " ['indication'],\n",
       " ['seriousminded', 'film'],\n",
       " ['seriousminded', 'film'],\n",
       " ['seriousminded', 'film'],\n",
       " ['seriousminded', 'film'],\n",
       " ['seriousminded'],\n",
       " ['film'],\n",
       " ['film'],\n",
       " ['plot', 'romantic', 'comedy', 'boilerplate', 'start', 'finish'],\n",
       " ['plot'],\n",
       " ['plot'],\n",
       " ['romantic', 'comedy', 'boilerplate', 'start', 'finish'],\n",
       " ['romantic', 'comedy', 'boilerplate', 'start', 'finish'],\n",
       " ['romantic', 'comedy', 'boilerplate', 'start', 'finish'],\n",
       " ['romantic', 'comedy', 'boilerplate', 'start'],\n",
       " ['romantic', 'comedy', 'boilerplate'],\n",
       " ['romantic'],\n",
       " ['comedy', 'boilerplate'],\n",
       " ['comedy'],\n",
       " ['boilerplate'],\n",
       " ['start'],\n",
       " ['start'],\n",
       " ['finish'],\n",
       " ['finish'],\n",
       " ['arrives',\n",
       "  'impeccable',\n",
       "  'pedigree',\n",
       "  'mongrel',\n",
       "  'pep',\n",
       "  'almost',\n",
       "  'indecipherable',\n",
       "  'plot',\n",
       "  'complications'],\n",
       " ['arrives',\n",
       "  'impeccable',\n",
       "  'pedigree',\n",
       "  'mongrel',\n",
       "  'pep',\n",
       "  'almost',\n",
       "  'indecipherable',\n",
       "  'plot',\n",
       "  'complications'],\n",
       " ['arrives',\n",
       "  'impeccable',\n",
       "  'pedigree',\n",
       "  'mongrel',\n",
       "  'pep',\n",
       "  'almost',\n",
       "  'indecipherable',\n",
       "  'plot',\n",
       "  'complications'],\n",
       " ['arrives'],\n",
       " ['impeccable',\n",
       "  'pedigree',\n",
       "  'mongrel',\n",
       "  'pep',\n",
       "  'almost',\n",
       "  'indecipherable',\n",
       "  'plot',\n",
       "  'complications'],\n",
       " ['impeccable',\n",
       "  'pedigree',\n",
       "  'mongrel',\n",
       "  'pep',\n",
       "  'almost',\n",
       "  'indecipherable',\n",
       "  'plot',\n",
       "  'complications'],\n",
       " ['impeccable', 'pedigree', 'mongrel', 'pep', 'almost'],\n",
       " ['impeccable', 'pedigree', 'mongrel', 'pep'],\n",
       " ['impeccable', 'pedigree', 'mongrel', 'pep'],\n",
       " ['impeccable', 'pedigree', 'mongrel', 'pep'],\n",
       " ['impeccable', 'pedigree', 'mongrel', 'pep'],\n",
       " ['impeccable'],\n",
       " ['pedigree', 'mongrel', 'pep'],\n",
       " ['pedigree'],\n",
       " ['mongrel', 'pep'],\n",
       " ['mongrel', 'pep'],\n",
       " ['mongrel'],\n",
       " ['pep'],\n",
       " ['almost'],\n",
       " ['indecipherable', 'plot', 'complications'],\n",
       " ['indecipherable'],\n",
       " ['plot', 'complications'],\n",
       " ['complications'],\n",
       " ['film', 'clearly', 'means', 'preach', 'exclusively', 'converted'],\n",
       " ['film', 'clearly', 'means'],\n",
       " ['film'],\n",
       " ['clearly', 'means'],\n",
       " ['clearly', 'means'],\n",
       " ['clearly'],\n",
       " ['means'],\n",
       " ['means'],\n",
       " ['preach', 'exclusively', 'converted'],\n",
       " ['preach', 'exclusively', 'converted'],\n",
       " ['preach', 'exclusively'],\n",
       " ['preach'],\n",
       " ['exclusively'],\n",
       " ['converted'],\n",
       " ['converted'],\n",
       " ['converted'],\n",
       " ['importance',\n",
       "  'earnest',\n",
       "  'offers',\n",
       "  'opportunities',\n",
       "  'occasional',\n",
       "  'smiles',\n",
       "  'chuckles',\n",
       "  'nt',\n",
       "  'give',\n",
       "  'us',\n",
       "  'reason',\n",
       "  'theater',\n",
       "  'beyond',\n",
       "  'wilde',\n",
       "  'wit',\n",
       "  'actors',\n",
       "  'performances'],\n",
       " ['importance',\n",
       "  'earnest',\n",
       "  'offers',\n",
       "  'opportunities',\n",
       "  'occasional',\n",
       "  'smiles',\n",
       "  'chuckles'],\n",
       " ['importance',\n",
       "  'earnest',\n",
       "  'offers',\n",
       "  'opportunities',\n",
       "  'occasional',\n",
       "  'smiles',\n",
       "  'chuckles'],\n",
       " ['importance', 'earnest'],\n",
       " ['earnest'],\n",
       " ['earnest'],\n",
       " ['offers', 'opportunities', 'occasional', 'smiles', 'chuckles'],\n",
       " ['offers', 'opportunities', 'occasional', 'smiles'],\n",
       " ['offers', 'opportunities', 'occasional', 'smiles'],\n",
       " ['offers'],\n",
       " ['opportunities', 'occasional', 'smiles'],\n",
       " ['opportunities'],\n",
       " ['occasional', 'smiles'],\n",
       " ['occasional', 'smiles'],\n",
       " ['occasional'],\n",
       " ['smiles'],\n",
       " ['chuckles'],\n",
       " ['nt',\n",
       "  'give',\n",
       "  'us',\n",
       "  'reason',\n",
       "  'theater',\n",
       "  'beyond',\n",
       "  'wilde',\n",
       "  'wit',\n",
       "  'actors',\n",
       "  'performances'],\n",
       " ['nt',\n",
       "  'give',\n",
       "  'us',\n",
       "  'reason',\n",
       "  'theater',\n",
       "  'beyond',\n",
       "  'wilde',\n",
       "  'wit',\n",
       "  'actors',\n",
       "  'performances'],\n",
       " ['nt',\n",
       "  'give',\n",
       "  'us',\n",
       "  'reason',\n",
       "  'theater',\n",
       "  'beyond',\n",
       "  'wilde',\n",
       "  'wit',\n",
       "  'actors',\n",
       "  'performances'],\n",
       " ['nt',\n",
       "  'give',\n",
       "  'us',\n",
       "  'reason',\n",
       "  'theater',\n",
       "  'beyond',\n",
       "  'wilde',\n",
       "  'wit',\n",
       "  'actors',\n",
       "  'performances'],\n",
       " ['give',\n",
       "  'us',\n",
       "  'reason',\n",
       "  'theater',\n",
       "  'beyond',\n",
       "  'wilde',\n",
       "  'wit',\n",
       "  'actors',\n",
       "  'performances'],\n",
       " ['give', 'us'],\n",
       " ['give'],\n",
       " ['reason', 'theater', 'beyond', 'wilde', 'wit', 'actors', 'performances'],\n",
       " ['reason', 'theater', 'beyond', 'wilde', 'wit', 'actors', 'performances'],\n",
       " ['theater', 'beyond', 'wilde', 'wit', 'actors', 'performances'],\n",
       " ['theater', 'beyond', 'wilde', 'wit', 'actors', 'performances'],\n",
       " ['theater'],\n",
       " ['theater'],\n",
       " ['theater'],\n",
       " ['theater'],\n",
       " ['beyond', 'wilde', 'wit', 'actors', 'performances'],\n",
       " ['beyond'],\n",
       " ['wilde', 'wit', 'actors', 'performances'],\n",
       " ['wilde', 'wit'],\n",
       " ['wilde', 'wit'],\n",
       " ['wilde'],\n",
       " ['wilde'],\n",
       " ['actors', 'performances'],\n",
       " ['actors'],\n",
       " ['actors'],\n",
       " ['actors'],\n",
       " ['latest',\n",
       "  'vapid',\n",
       "  'actor',\n",
       "  'exercise',\n",
       "  'appropriate',\n",
       "  'structure',\n",
       "  'arthur',\n",
       "  'schnitzler',\n",
       "  'reigen'],\n",
       " ['latest'],\n",
       " ['latest'],\n",
       " ['vapid',\n",
       "  'actor',\n",
       "  'exercise',\n",
       "  'appropriate',\n",
       "  'structure',\n",
       "  'arthur',\n",
       "  'schnitzler',\n",
       "  'reigen'],\n",
       " ['vapid',\n",
       "  'actor',\n",
       "  'exercise',\n",
       "  'appropriate',\n",
       "  'structure',\n",
       "  'arthur',\n",
       "  'schnitzler',\n",
       "  'reigen'],\n",
       " ['vapid', 'actor', 'exercise'],\n",
       " ['vapid'],\n",
       " ['actor', 'exercise'],\n",
       " ['actor'],\n",
       " ['actor'],\n",
       " ['exercise'],\n",
       " ['appropriate', 'structure', 'arthur', 'schnitzler', 'reigen'],\n",
       " ['appropriate', 'structure', 'arthur', 'schnitzler', 'reigen'],\n",
       " ['appropriate', 'structure'],\n",
       " ['appropriate'],\n",
       " ['structure'],\n",
       " ['structure'],\n",
       " ['arthur', 'schnitzler', 'reigen'],\n",
       " ['arthur', 'schnitzler', 'reigen'],\n",
       " ['arthur', 'schnitzler'],\n",
       " ['arthur'],\n",
       " ['schnitzler'],\n",
       " ['schnitzler'],\n",
       " ['reigen'],\n",
       " ['vaudeville',\n",
       "  'show',\n",
       "  'wellconstructed',\n",
       "  'narrative',\n",
       "  'terms',\n",
       "  'inoffensive',\n",
       "  'actually',\n",
       "  'rather',\n",
       "  'sweet'],\n",
       " ['vaudeville',\n",
       "  'show',\n",
       "  'wellconstructed',\n",
       "  'narrative',\n",
       "  'terms',\n",
       "  'inoffensive',\n",
       "  'actually',\n",
       "  'rather',\n",
       "  'sweet'],\n",
       " ['vaudeville', 'show', 'wellconstructed', 'narrative'],\n",
       " ['vaudeville', 'show', 'wellconstructed', 'narrative'],\n",
       " ['vaudeville', 'show', 'wellconstructed', 'narrative'],\n",
       " ['vaudeville'],\n",
       " ['vaudeville'],\n",
       " ['show', 'wellconstructed', 'narrative'],\n",
       " ['show'],\n",
       " ['wellconstructed', 'narrative'],\n",
       " ['wellconstructed', 'narrative'],\n",
       " ['wellconstructed'],\n",
       " ['narrative'],\n",
       " ['terms', 'inoffensive', 'actually', 'rather', 'sweet'],\n",
       " ['terms'],\n",
       " ['terms'],\n",
       " ['terms'],\n",
       " ['inoffensive', 'actually', 'rather', 'sweet'],\n",
       " ['inoffensive', 'actually', 'rather', 'sweet'],\n",
       " ['inoffensive', 'actually', 'rather', 'sweet'],\n",
       " ['inoffensive', 'actually'],\n",
       " ['inoffensive'],\n",
       " ['inoffensive'],\n",
       " ['actually'],\n",
       " ['rather', 'sweet'],\n",
       " ['rather'],\n",
       " ['nothing', 'runofthemill', 'action', 'flick'],\n",
       " ['runofthemill', 'action', 'flick'],\n",
       " ['runofthemill', 'action'],\n",
       " ['runofthemill', 'action'],\n",
       " ['runofthemill', 'action'],\n",
       " ['runofthemill', 'action'],\n",
       " ['runofthemill'],\n",
       " ['action'],\n",
       " ['flick'],\n",
       " ['hampered',\n",
       "  'paralyzed',\n",
       "  'selfindulgent',\n",
       "  'script',\n",
       "  'aims',\n",
       "  'poetry',\n",
       "  'ends',\n",
       "  'sounding',\n",
       "  'like',\n",
       "  'satire'],\n",
       " ['hampered',\n",
       "  'paralyzed',\n",
       "  'selfindulgent',\n",
       "  'script',\n",
       "  'aims',\n",
       "  'poetry',\n",
       "  'ends',\n",
       "  'sounding',\n",
       "  'like',\n",
       "  'satire'],\n",
       " ['hampered', 'paralyzed', 'selfindulgent', 'script'],\n",
       " ['hampered', 'paralyzed', 'selfindulgent', 'script'],\n",
       " ['hampered', 'paralyzed'],\n",
       " ['hampered'],\n",
       " ['paralyzed'],\n",
       " ['paralyzed'],\n",
       " ['paralyzed'],\n",
       " ['paralyzed'],\n",
       " ['paralyzed'],\n",
       " ['selfindulgent', 'script'],\n",
       " ['selfindulgent', 'script'],\n",
       " ['selfindulgent', 'script'],\n",
       " ['selfindulgent'],\n",
       " ['script'],\n",
       " ['aims', 'poetry', 'ends', 'sounding', 'like', 'satire'],\n",
       " ['aims', 'poetry', 'ends', 'sounding', 'like', 'satire'],\n",
       " ['aims', 'poetry'],\n",
       " ['aims', 'poetry'],\n",
       " ['aims'],\n",
       " ['poetry'],\n",
       " ['poetry'],\n",
       " ['ends', 'sounding', 'like', 'satire'],\n",
       " ['ends'],\n",
       " ['ends'],\n",
       " ['sounding', 'like', 'satire'],\n",
       " ['sounding'],\n",
       " ['like', 'satire'],\n",
       " ['satire'],\n",
       " ['ice',\n",
       "  'age',\n",
       "  'first',\n",
       "  'computergenerated',\n",
       "  'feature',\n",
       "  'cartoon',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'movies',\n",
       "  'makes',\n",
       "  'glacial',\n",
       "  'pacing',\n",
       "  'early'],\n",
       " ['ice',\n",
       "  'age',\n",
       "  'first',\n",
       "  'computergenerated',\n",
       "  'feature',\n",
       "  'cartoon',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'movies',\n",
       "  'makes',\n",
       "  'glacial',\n",
       "  'pacing',\n",
       "  'early'],\n",
       " ['ice',\n",
       "  'age',\n",
       "  'first',\n",
       "  'computergenerated',\n",
       "  'feature',\n",
       "  'cartoon',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'movies'],\n",
       " ['ice',\n",
       "  'age',\n",
       "  'first',\n",
       "  'computergenerated',\n",
       "  'feature',\n",
       "  'cartoon',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'movies'],\n",
       " ['ice',\n",
       "  'age',\n",
       "  'first',\n",
       "  'computergenerated',\n",
       "  'feature',\n",
       "  'cartoon',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'movies'],\n",
       " ['ice', 'age'],\n",
       " ['ice'],\n",
       " ['age'],\n",
       " ['first',\n",
       "  'computergenerated',\n",
       "  'feature',\n",
       "  'cartoon',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'movies'],\n",
       " ['first',\n",
       "  'computergenerated',\n",
       "  'feature',\n",
       "  'cartoon',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'movies'],\n",
       " ['first', 'computergenerated', 'feature', 'cartoon'],\n",
       " ['first', 'computergenerated', 'feature', 'cartoon'],\n",
       " ['first'],\n",
       " ['computergenerated', 'feature', 'cartoon'],\n",
       " ['computergenerated'],\n",
       " ['feature', 'cartoon'],\n",
       " ['feature'],\n",
       " ['cartoon'],\n",
       " ['feel', 'like', 'movies'],\n",
       " ['feel', 'like', 'movies'],\n",
       " ['feel'],\n",
       " ['like', 'movies'],\n",
       " ['movies'],\n",
       " ['makes', 'glacial', 'pacing', 'early'],\n",
       " ['makes', 'glacial', 'pacing', 'early'],\n",
       " ['makes', 'glacial', 'pacing'],\n",
       " ['makes'],\n",
       " ['glacial', 'pacing'],\n",
       " ['glacial', 'pacing'],\n",
       " ['glacial', 'pacing'],\n",
       " ['glacial'],\n",
       " ['pacing'],\n",
       " ['early'],\n",
       " ['early'],\n",
       " ['little',\n",
       "  'sense',\n",
       "  'going',\n",
       "  'makers',\n",
       "  'serve',\n",
       "  'cliches',\n",
       "  'considerable',\n",
       "  'dash'],\n",
       " ['little',\n",
       "  'sense',\n",
       "  'going',\n",
       "  'makers',\n",
       "  'serve',\n",
       "  'cliches',\n",
       "  'considerable',\n",
       "  'dash'],\n",
       " ['little', 'sense', 'going'],\n",
       " ['little', 'sense', 'going'],\n",
       " ['little', 'sense', 'going'],\n",
       " ['little', 'sense', 'going'],\n",
       " ['little', 'sense', 'going'],\n",
       " ['little', 'sense'],\n",
       " ['little'],\n",
       " ['going'],\n",
       " ['going'],\n",
       " ['going'],\n",
       " ['going'],\n",
       " ['going'],\n",
       " ['makers', 'serve', 'cliches', 'considerable', 'dash'],\n",
       " ['makers'],\n",
       " ['makers'],\n",
       " ['serve', 'cliches', 'considerable', 'dash'],\n",
       " ['serve'],\n",
       " ['serve'],\n",
       " ['cliches', 'considerable', 'dash'],\n",
       " ['cliches'],\n",
       " ['cliches'],\n",
       " ['considerable', 'dash'],\n",
       " ['considerable', 'dash'],\n",
       " ['considerable'],\n",
       " ['dash'],\n",
       " ['cattaneo',\n",
       "  'followed',\n",
       "  'runaway',\n",
       "  'success',\n",
       "  'first',\n",
       "  'film',\n",
       "  'full',\n",
       "  'monty',\n",
       "  'something',\n",
       "  'different'],\n",
       " ['cattaneo'],\n",
       " ['followed',\n",
       "  'runaway',\n",
       "  'success',\n",
       "  'first',\n",
       "  'film',\n",
       "  'full',\n",
       "  'monty',\n",
       "  'something',\n",
       "  'different'],\n",
       " ['followed',\n",
       "  'runaway',\n",
       "  'success',\n",
       "  'first',\n",
       "  'film',\n",
       "  'full',\n",
       "  'monty',\n",
       "  'something',\n",
       "  'different'],\n",
       " ['followed',\n",
       "  'runaway',\n",
       "  'success',\n",
       "  'first',\n",
       "  'film',\n",
       "  'full',\n",
       "  'monty',\n",
       "  'something',\n",
       "  'different'],\n",
       " ['followed',\n",
       "  'runaway',\n",
       "  'success',\n",
       "  'first',\n",
       "  'film',\n",
       "  'full',\n",
       "  'monty',\n",
       "  'something',\n",
       "  'different'],\n",
       " ['followed', 'runaway', 'success', 'first', 'film', 'full', 'monty'],\n",
       " ['followed'],\n",
       " ['runaway', 'success', 'first', 'film', 'full', 'monty'],\n",
       " ['runaway', 'success'],\n",
       " ['runaway', 'success'],\n",
       " ['runaway'],\n",
       " ['success'],\n",
       " ['first', 'film', 'full', 'monty'],\n",
       " ['first', 'film', 'full', 'monty'],\n",
       " ['first', 'film', 'full', 'monty'],\n",
       " ['first', 'film'],\n",
       " ['first', 'film'],\n",
       " ['first', 'film'],\n",
       " ['full', 'monty'],\n",
       " ['full', 'monty'],\n",
       " ['full'],\n",
       " ['monty'],\n",
       " ['something', 'different'],\n",
       " ['something', 'different'],\n",
       " ['different'],\n",
       " ['unnamed',\n",
       "  'easily',\n",
       "  'substitutable',\n",
       "  'forces',\n",
       "  'serve',\n",
       "  'whatever',\n",
       "  'terror',\n",
       "  'heroes',\n",
       "  'horror',\n",
       "  'movies',\n",
       "  'try',\n",
       "  'avoid'],\n",
       " ['unnamed',\n",
       "  'easily',\n",
       "  'substitutable',\n",
       "  'forces',\n",
       "  'serve',\n",
       "  'whatever',\n",
       "  'terror',\n",
       "  'heroes',\n",
       "  'horror',\n",
       "  'movies',\n",
       "  'try',\n",
       "  'avoid'],\n",
       " ['unnamed',\n",
       "  'easily',\n",
       "  'substitutable',\n",
       "  'forces',\n",
       "  'serve',\n",
       "  'whatever',\n",
       "  'terror',\n",
       "  'heroes',\n",
       "  'horror',\n",
       "  'movies',\n",
       "  'try',\n",
       "  'avoid'],\n",
       " ['unnamed',\n",
       "  'easily',\n",
       "  'substitutable',\n",
       "  'forces',\n",
       "  'serve',\n",
       "  'whatever',\n",
       "  'terror',\n",
       "  'heroes',\n",
       "  'horror',\n",
       "  'movies',\n",
       "  'try',\n",
       "  'avoid'],\n",
       " ['unnamed', 'easily', 'substitutable', 'forces'],\n",
       " ['unnamed', 'easily', 'substitutable', 'forces'],\n",
       " ['unnamed', 'easily', 'substitutable'],\n",
       " ['unnamed'],\n",
       " ['easily', 'substitutable'],\n",
       " ['easily', 'substitutable'],\n",
       " ['easily'],\n",
       " ['substitutable'],\n",
       " ['forces'],\n",
       " ['serve', 'whatever', 'terror', 'heroes', 'horror', 'movies', 'try', 'avoid'],\n",
       " ['serve', 'whatever', 'terror', 'heroes', 'horror', 'movies', 'try', 'avoid'],\n",
       " ['whatever', 'terror', 'heroes', 'horror', 'movies', 'try', 'avoid'],\n",
       " ['whatever', 'terror', 'heroes', 'horror', 'movies', 'try', 'avoid'],\n",
       " ['whatever'],\n",
       " ['terror', 'heroes', 'horror', 'movies', 'try', 'avoid'],\n",
       " ['terror'],\n",
       " ['heroes', 'horror', 'movies', 'try', 'avoid'],\n",
       " ['heroes', 'horror', 'movies'],\n",
       " ['heroes'],\n",
       " ['heroes'],\n",
       " ['horror', 'movies'],\n",
       " ['horror', 'movies'],\n",
       " ['horror'],\n",
       " ['try', 'avoid'],\n",
       " ['avoid'],\n",
       " ['avoid'],\n",
       " ['almost', 'feels', 'movie', 'interested', 'entertaining', 'amusing', 'us'],\n",
       " ['almost', 'feels', 'movie', 'interested', 'entertaining', 'amusing', 'us'],\n",
       " ['feels', 'movie', 'interested', 'entertaining', 'amusing', 'us'],\n",
       " ['feels', 'movie', 'interested', 'entertaining', 'amusing', 'us'],\n",
       " ['feels'],\n",
       " ['movie', 'interested', 'entertaining', 'amusing', 'us'],\n",
       " ['movie', 'interested', 'entertaining', 'amusing', 'us'],\n",
       " ['movie', 'interested', 'entertaining', 'amusing', 'us'],\n",
       " ['interested', 'entertaining', 'amusing', 'us'],\n",
       " ['interested', 'entertaining', 'amusing', 'us'],\n",
       " ['interested', 'entertaining'],\n",
       " ['interested', 'entertaining'],\n",
       " ['interested'],\n",
       " ['entertaining'],\n",
       " ['entertaining'],\n",
       " ['amusing', 'us'],\n",
       " ['amusing', 'us'],\n",
       " ['amusing', 'us'],\n",
       " ['amusing'],\n",
       " ['movie',\n",
       "  'progression',\n",
       "  'rambling',\n",
       "  'incoherence',\n",
       "  'gives',\n",
       "  'new',\n",
       "  'meaning',\n",
       "  'phrase',\n",
       "  'fatal',\n",
       "  'script',\n",
       "  'error'],\n",
       " ['movie', 'progression', 'rambling', 'incoherence'],\n",
       " ['movie', 'progression'],\n",
       " ['progression'],\n",
       " ['rambling', 'incoherence'],\n",
       " ['rambling', 'incoherence'],\n",
       " ['rambling'],\n",
       " ['incoherence'],\n",
       " ['gives', 'new', 'meaning', 'phrase', 'fatal', 'script', 'error'],\n",
       " ['gives', 'new', 'meaning', 'phrase', 'fatal', 'script', 'error'],\n",
       " ['gives', 'new', 'meaning', 'phrase', 'fatal', 'script', 'error'],\n",
       " ['gives', 'new', 'meaning'],\n",
       " ['gives'],\n",
       " ['new', 'meaning'],\n",
       " ['new'],\n",
       " ['meaning'],\n",
       " ['phrase', 'fatal', 'script', 'error'],\n",
       " ['phrase', 'fatal', 'script', 'error'],\n",
       " ['phrase', 'fatal', 'script', 'error'],\n",
       " ['phrase'],\n",
       " ['fatal', 'script', 'error'],\n",
       " ['fatal', 'script', 'error'],\n",
       " ['fatal'],\n",
       " ['script', 'error'],\n",
       " ['error'],\n",
       " ['nt',\n",
       "  'judge',\n",
       "  'one',\n",
       "  'soon',\n",
       "  'dark',\n",
       "  'gritty',\n",
       "  'story',\n",
       "  'takes',\n",
       "  'totally',\n",
       "  'unexpected',\n",
       "  'directions',\n",
       "  'keeps',\n",
       "  'going'],\n",
       " ['nt',\n",
       "  'judge',\n",
       "  'one',\n",
       "  'soon',\n",
       "  'dark',\n",
       "  'gritty',\n",
       "  'story',\n",
       "  'takes',\n",
       "  'totally',\n",
       "  'unexpected',\n",
       "  'directions',\n",
       "  'keeps',\n",
       "  'going'],\n",
       " ['nt', 'judge', 'one', 'soon'],\n",
       " ['nt', 'judge', 'one', 'soon'],\n",
       " ['nt'],\n",
       " ['judge', 'one', 'soon'],\n",
       " ['judge', 'one'],\n",
       " ['judge'],\n",
       " ['soon'],\n",
       " ['soon'],\n",
       " ['dark',\n",
       "  'gritty',\n",
       "  'story',\n",
       "  'takes',\n",
       "  'totally',\n",
       "  'unexpected',\n",
       "  'directions',\n",
       "  'keeps',\n",
       "  'going'],\n",
       " ['dark', 'gritty', 'story'],\n",
       " ['dark', 'gritty', 'story'],\n",
       " ['dark', 'gritty', 'story'],\n",
       " ['dark', 'gritty', 'story'],\n",
       " ['dark', 'gritty', 'story'],\n",
       " ['dark'],\n",
       " ['gritty', 'story'],\n",
       " ['gritty', 'story'],\n",
       " ['gritty'],\n",
       " ['takes', 'totally', 'unexpected', 'directions', 'keeps', 'going'],\n",
       " ['takes', 'totally', 'unexpected', 'directions', 'keeps', 'going'],\n",
       " ['takes', 'totally', 'unexpected', 'directions'],\n",
       " ['takes', 'totally', 'unexpected', 'directions'],\n",
       " ['takes'],\n",
       " ['takes'],\n",
       " ['totally', 'unexpected', 'directions'],\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b58c04f5-4598-407d-989f-e98e618e7694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus, vector_size=100, window=5, min_count=2, epochs=30, sg=1, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e565101e-7335-4544-834d-378d7481537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_w2v(tokens, model, vector_size=100):\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv.key_to_index:\n",
    "            vectors.append(model.wv[token])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a89a8a94-5778-41b3-8919-3df9816ff060",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['w2v_vector'] = train_df['Phrase_tokens_filtered'].apply(lambda x: text_to_w2v(x, model, vector_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "577d0780-60f2-44d0-8242-1fb4e50987b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_clean</th>\n",
       "      <th>Phrase_tokens</th>\n",
       "      <th>Phrase_tokens_filtered</th>\n",
       "      <th>text_for_tfidf</th>\n",
       "      <th>w2v_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>[a, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[series, escapades, demonstrating, adage, good...</td>\n",
       "      <td>series escapades demonstrating adage good goos...</td>\n",
       "      <td>[-0.20092095, 0.86766917, 0.051330924, -0.3131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>[a, series, of, escapades, demonstrating, the,...</td>\n",
       "      <td>[series, escapades, demonstrating, adage, good...</td>\n",
       "      <td>series escapades demonstrating adage good goose</td>\n",
       "      <td>[-0.2032486, 0.85805845, 0.010760595, -0.11542...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>a series</td>\n",
       "      <td>[a, series]</td>\n",
       "      <td>[series]</td>\n",
       "      <td>series</td>\n",
       "      <td>[0.6506634, 0.7794791, 0.5006125, -0.07772939,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>series</td>\n",
       "      <td>[series]</td>\n",
       "      <td>[series]</td>\n",
       "      <td>series</td>\n",
       "      <td>[0.6506634, 0.7794791, 0.5006125, -0.07772939,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156055</th>\n",
       "      <td>Hearst 's</td>\n",
       "      <td>2</td>\n",
       "      <td>hearst s</td>\n",
       "      <td>[hearst, s]</td>\n",
       "      <td>[hearst]</td>\n",
       "      <td>hearst</td>\n",
       "      <td>[-0.77291733, 0.3958901, -0.11356394, -0.48875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156056</th>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>1</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>[forced, avuncular, chortles]</td>\n",
       "      <td>[forced, avuncular, chortles]</td>\n",
       "      <td>forced avuncular chortles</td>\n",
       "      <td>[-0.23821314, 0.4391351, 0.4365277, -0.2969419...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156057</th>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>3</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>[avuncular, chortles]</td>\n",
       "      <td>[avuncular, chortles]</td>\n",
       "      <td>avuncular chortles</td>\n",
       "      <td>[-0.32514876, 0.32190543, 0.059753947, -0.1686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156058</th>\n",
       "      <td>avuncular</td>\n",
       "      <td>2</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>[avuncular]</td>\n",
       "      <td>[avuncular]</td>\n",
       "      <td>avuncular</td>\n",
       "      <td>[-0.35260728, 0.323849, 0.057352215, -0.116505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156059</th>\n",
       "      <td>chortles</td>\n",
       "      <td>2</td>\n",
       "      <td>chortles</td>\n",
       "      <td>[chortles]</td>\n",
       "      <td>[chortles]</td>\n",
       "      <td>chortles</td>\n",
       "      <td>[-0.29769027, 0.3199619, 0.06215568, -0.220717...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156060 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Phrase  Sentiment  \\\n",
       "0       A series of escapades demonstrating the adage ...          1   \n",
       "1       A series of escapades demonstrating the adage ...          2   \n",
       "2                                                A series          2   \n",
       "3                                                       A          2   \n",
       "4                                                  series          2   \n",
       "...                                                   ...        ...   \n",
       "156055                                          Hearst 's          2   \n",
       "156056                          forced avuncular chortles          1   \n",
       "156057                                 avuncular chortles          3   \n",
       "156058                                          avuncular          2   \n",
       "156059                                           chortles          2   \n",
       "\n",
       "                                             Phrase_clean  \\\n",
       "0       a series of escapades demonstrating the adage ...   \n",
       "1       a series of escapades demonstrating the adage ...   \n",
       "2                                                a series   \n",
       "3                                                       a   \n",
       "4                                                  series   \n",
       "...                                                   ...   \n",
       "156055                                           hearst s   \n",
       "156056                          forced avuncular chortles   \n",
       "156057                                 avuncular chortles   \n",
       "156058                                          avuncular   \n",
       "156059                                           chortles   \n",
       "\n",
       "                                            Phrase_tokens  \\\n",
       "0       [a, series, of, escapades, demonstrating, the,...   \n",
       "1       [a, series, of, escapades, demonstrating, the,...   \n",
       "2                                             [a, series]   \n",
       "3                                                     [a]   \n",
       "4                                                [series]   \n",
       "...                                                   ...   \n",
       "156055                                        [hearst, s]   \n",
       "156056                      [forced, avuncular, chortles]   \n",
       "156057                              [avuncular, chortles]   \n",
       "156058                                        [avuncular]   \n",
       "156059                                         [chortles]   \n",
       "\n",
       "                                   Phrase_tokens_filtered  \\\n",
       "0       [series, escapades, demonstrating, adage, good...   \n",
       "1       [series, escapades, demonstrating, adage, good...   \n",
       "2                                                [series]   \n",
       "3                                                      []   \n",
       "4                                                [series]   \n",
       "...                                                   ...   \n",
       "156055                                           [hearst]   \n",
       "156056                      [forced, avuncular, chortles]   \n",
       "156057                              [avuncular, chortles]   \n",
       "156058                                        [avuncular]   \n",
       "156059                                         [chortles]   \n",
       "\n",
       "                                           text_for_tfidf  \\\n",
       "0       series escapades demonstrating adage good goos...   \n",
       "1         series escapades demonstrating adage good goose   \n",
       "2                                                  series   \n",
       "3                                                           \n",
       "4                                                  series   \n",
       "...                                                   ...   \n",
       "156055                                             hearst   \n",
       "156056                          forced avuncular chortles   \n",
       "156057                                 avuncular chortles   \n",
       "156058                                          avuncular   \n",
       "156059                                           chortles   \n",
       "\n",
       "                                               w2v_vector  \n",
       "0       [-0.20092095, 0.86766917, 0.051330924, -0.3131...  \n",
       "1       [-0.2032486, 0.85805845, 0.010760595, -0.11542...  \n",
       "2       [0.6506634, 0.7794791, 0.5006125, -0.07772939,...  \n",
       "3       [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4       [0.6506634, 0.7794791, 0.5006125, -0.07772939,...  \n",
       "...                                                   ...  \n",
       "156055  [-0.77291733, 0.3958901, -0.11356394, -0.48875...  \n",
       "156056  [-0.23821314, 0.4391351, 0.4365277, -0.2969419...  \n",
       "156057  [-0.32514876, 0.32190543, 0.059753947, -0.1686...  \n",
       "156058  [-0.35260728, 0.323849, 0.057352215, -0.116505...  \n",
       "156059  [-0.29769027, 0.3199619, 0.06215568, -0.220717...  \n",
       "\n",
       "[156060 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b36264a-3c78-4026-9e97-f7d6459859b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(train_df['w2v_vector'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9cdf954-9cdc-4cc1-9aa8-cd4347aa8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2625cfc3-a2bd-4c5e-918b-eae50cf0779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(train_df['Sentiment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46b38df5-f07b-4cb6-829d-503925a0a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e1dd57f-c489-4b3d-9e4a-bc9d5178698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-title",
   "metadata": {},
   "source": [
    "---\n",
    "## PyTorch BiLSTM 情感分类模型\n",
    "\n",
    "在前面文本清洗 / 分词 / Word2Vec 的基础上，用 **PyTorch** 构建双向 LSTM 完成 5 分类情感分析。\n",
    "\n",
    "流程：`Phrase_clean` → 自定义 `Vocabulary` → `DataLoader` → `Embedding + BiLSTM×2 + Linear` → `CrossEntropyLoss` → **GPU 训练**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s1",
   "metadata": {},
   "source": [
    "### 1. 依赖导入 & 设备检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "pt-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "GPU 型号: NVIDIA GeForce RTX 2060\n",
      "显存总量: 6.4 GB\n"
     ]
    }
   ],
   "source": [
    "import re, string, pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# ── 设备选择：优先 CUDA，其次 MPS（Apple Silicon），最后 CPU ──\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f'使用设备: {DEVICE}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'GPU 型号: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'显存总量: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s2",
   "metadata": {},
   "source": [
    "### 2. 超参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "pt-hparams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 超参数 ────────────────────────────────────────────────────\n",
    "MAX_VOCAB   = 20000   # 词表大小\n",
    "MAX_LEN     = 50      # 序列最大长度（影评短语通常不超过 50 词）\n",
    "EMBED_DIM   = 128     # Embedding 维度\n",
    "HIDDEN_DIM  = 256     # LSTM 隐层维度（双向后输出 512）\n",
    "NUM_LAYERS  = 2       # LSTM 层数\n",
    "DROPOUT     = 0.3\n",
    "NUM_CLASSES = 5       # 情感类别 0~4\n",
    "BATCH_SIZE  = 256\n",
    "LR          = 1e-3\n",
    "EPOCHS      = 15\n",
    "CLIP_GRAD   = 5.0     # 梯度裁剪阈值，防止梯度爆炸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s3",
   "metadata": {},
   "source": [
    "### 3. 构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "pt-vocab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词表大小: 16404\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"将词列表映射为整数索引，支持 <PAD>=0 和 <UNK>=1。\"\"\"\n",
    "    PAD, UNK = 0, 1\n",
    "\n",
    "    def __init__(self, max_size=None):\n",
    "        self.max_size = max_size\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "\n",
    "    def build(self, token_lists):\n",
    "        \"\"\"token_lists: List[List[str]]\"\"\"\n",
    "        counter = Counter(tok for tokens in token_lists for tok in tokens)\n",
    "        most_common = counter.most_common(\n",
    "            self.max_size - 2 if self.max_size else None\n",
    "        )\n",
    "        for word, _ in most_common:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx]  = word\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"List[str] -> List[int]\"\"\"\n",
    "        return [self.word2idx.get(t, self.UNK) for t in tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "# 使用 Phrase_tokens（已分词列表）构建词表\n",
    "vocab = Vocabulary(max_size=MAX_VOCAB)\n",
    "vocab.build(train_df['Phrase_tokens'].tolist())\n",
    "print(f'词表大小: {len(vocab)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s4",
   "metadata": {},
   "source": [
    "### 4. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "pt-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本: 124,848  验证样本: 31,212\n",
      "训练批次: 488  验证批次: 61\n"
     ]
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        \"\"\"\n",
    "        texts  : List[List[str]]  已分词 token 列表\n",
    "        labels : np.ndarray       整数标签 0~4\n",
    "        \"\"\"\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        # 编码 + 截断\n",
    "        self.encoded = [\n",
    "            torch.tensor(vocab.encode(toks[:max_len]), dtype=torch.long)\n",
    "            for toks in texts\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"动态 padding 到 batch 内最长序列，同时返回真实长度供 pack 使用。\"\"\"\n",
    "    seqs, labels = zip(*batch)\n",
    "    lengths = torch.tensor([max(len(s), 1) for s in seqs], dtype=torch.long)\n",
    "    padded  = pad_sequence(seqs, batch_first=True, padding_value=Vocabulary.PAD)\n",
    "    return padded, lengths, torch.stack(labels)\n",
    "\n",
    "\n",
    "# ── 划分训练 / 验证集 ─────────────────────────────────────────\n",
    "texts_list = train_df['Phrase_tokens'].tolist()\n",
    "labels_arr = train_df['Sentiment'].values.astype(int)\n",
    "\n",
    "assert labels_arr.min() == 0 and labels_arr.max() == 4, '标签需为 0~4'\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    texts_list, labels_arr,\n",
    "    test_size=0.2, random_state=42, stratify=labels_arr\n",
    ")\n",
    "\n",
    "train_ds = SentimentDataset(X_tr, y_tr, vocab, MAX_LEN)\n",
    "val_ds   = SentimentDataset(X_val, y_val, vocab, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=collate_fn, num_workers=2,\n",
    "    pin_memory=(DEVICE.type == 'cuda')\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE * 2, shuffle=False,\n",
    "    collate_fn=collate_fn, num_workers=2,\n",
    "    pin_memory=(DEVICE.type == 'cuda')\n",
    ")\n",
    "\n",
    "print(f'训练样本: {len(train_ds):,}  验证样本: {len(val_ds):,}')\n",
    "print(f'训练批次: {len(train_loader)}  验证批次: {len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s5",
   "metadata": {},
   "source": [
    "### 5. 模型定义 —— PyTorch BiLSTM\n",
    "\n",
    "```\n",
    "Embedding(vocab_size, embed_dim)          # 词向量查找表\n",
    "    ↓\n",
    "BiLSTM-1 (hidden_dim=256, bidirectional)  # 输出 512 维序列\n",
    "    ↓ Dropout\n",
    "BiLSTM-2 (hidden_dim=128, bidirectional)  # 输出 256 维序列\n",
    "    ↓ 取真实最后时刻隐状态\n",
    "LayerNorm → Linear(256→64) → ReLU → Dropout → Linear(64→5)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "pt-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMClassifier(\n",
      "  (embedding): Embedding(16404, 128, padding_idx=0)\n",
      "  (lstm1): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
      "  (lstm2): LSTM(512, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "可训练参数量: 3,564,933\n",
      "模型所在设备: cuda:0\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim,\n",
    "                 num_layers, num_classes, dropout, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, embed_dim, padding_idx=pad_idx\n",
    "        )\n",
    "\n",
    "        # 第一层 BiLSTM：embed_dim -> hidden_dim*2\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            embed_dim, hidden_dim,\n",
    "            num_layers=1, batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        # 第二层 BiLSTM：hidden_dim*2 -> hidden_dim\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            hidden_dim * 2, hidden_dim // 2,\n",
    "            num_layers=1, batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout    = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)  # hidden_dim//2 * 2 = hidden_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (B, T)   lengths: (B,) 真实序列长度\n",
    "        emb = self.dropout(self.embedding(x))           # (B, T, E)\n",
    "\n",
    "        # ── LSTM-1 ──\n",
    "        packed1      = pack_padded_sequence(emb, lengths.cpu(),\n",
    "                                            batch_first=True, enforce_sorted=False)\n",
    "        out1, _      = self.lstm1(packed1)\n",
    "        out1, _      = pad_packed_sequence(out1, batch_first=True)  # (B, T, H*2)\n",
    "        out1         = self.dropout(out1)\n",
    "\n",
    "        # ── LSTM-2 ──\n",
    "        packed2      = pack_padded_sequence(out1, lengths.cpu(),\n",
    "                                            batch_first=True, enforce_sorted=False)\n",
    "        out2, _      = self.lstm2(packed2)\n",
    "        out2, _      = pad_packed_sequence(out2, batch_first=True)  # (B, T, H)\n",
    "\n",
    "        # 取每条序列真实最后一个时刻的输出（避免取到 padding 位置）\n",
    "        idx      = (lengths - 1).clamp(min=0).to(x.device)\n",
    "        last_out = out2[torch.arange(out2.size(0), device=x.device), idx]  # (B, H)\n",
    "\n",
    "        out = self.layer_norm(last_out)\n",
    "        return self.classifier(out)                     # (B, num_classes)\n",
    "\n",
    "\n",
    "model = BiLSTMClassifier(\n",
    "    vocab_size  = len(vocab),\n",
    "    embed_dim   = EMBED_DIM,\n",
    "    hidden_dim  = HIDDEN_DIM,\n",
    "    num_layers  = NUM_LAYERS,\n",
    "    num_classes = NUM_CLASSES,\n",
    "    dropout     = DROPOUT,\n",
    "    pad_idx     = Vocabulary.PAD\n",
    ").to(DEVICE)   # ← 模型整体移至 GPU\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(model)\n",
    "print(f'\\n可训练参数量: {total_params:,}')\n",
    "print(f'模型所在设备: {next(model.parameters()).device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s6",
   "metadata": {},
   "source": [
    "### 6. 损失函数 / 优化器 / 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "pt-train-utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "\n",
    "def run_epoch(loader, model, criterion, optimizer=None, clip=None):\n",
    "    \"\"\"单轮训练或评估。optimizer=None 时进入评估模式。\"\"\"\n",
    "    is_train = optimizer is not None\n",
    "    model.train() if is_train else model.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "    ctx = torch.enable_grad() if is_train else torch.no_grad()\n",
    "\n",
    "    with ctx:\n",
    "        for seqs, lengths, labels in loader:\n",
    "            # 数据迁移至 GPU（non_blocking 与 pin_memory 配合异步传输）\n",
    "            seqs    = seqs.to(DEVICE, non_blocking=True)\n",
    "            lengths = lengths.to(DEVICE, non_blocking=True)\n",
    "            labels  = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "            logits = model(seqs, lengths)           # 前向传播\n",
    "            loss   = criterion(logits, labels)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()                     # 反向传播\n",
    "                if clip:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "\n",
    "            preds          = logits.argmax(dim=1)\n",
    "            total_loss    += loss.item() * labels.size(0)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    return total_loss / total_samples, total_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s7",
   "metadata": {},
   "source": [
    "### 7. 训练主循环（GPU 加速）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pt-train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience_cnt = 0\n",
    "EARLY_STOP   = 4      # 连续 4 轮验证 acc 不提升则停止\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [],\n",
    "           'train_acc':  [], 'val_acc':  []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, model, criterion,\n",
    "                                optimizer=optimizer, clip=CLIP_GRAD)\n",
    "    vl_loss, vl_acc = run_epoch(val_loader,   model, criterion)\n",
    "\n",
    "    scheduler.step(vl_acc)\n",
    "\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['val_loss'].append(vl_loss)\n",
    "    history['train_acc'].append(tr_acc)\n",
    "    history['val_acc'].append(vl_acc)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f'Epoch {epoch:02d}/{EPOCHS}  '\n",
    "          f'Train Loss: {tr_loss:.4f}  Acc: {tr_acc:.4f}  |  '\n",
    "          f'Val Loss: {vl_loss:.4f}  Acc: {vl_acc:.4f}  '\n",
    "          f'({elapsed:.1f}s)')\n",
    "\n",
    "    # EarlyStopping：保存最佳权重\n",
    "    if vl_acc > best_val_acc:\n",
    "        best_val_acc = vl_acc\n",
    "        torch.save(model.state_dict(), 'best_bilstm.pt')\n",
    "        patience_cnt = 0\n",
    "    else:\n",
    "        patience_cnt += 1\n",
    "        if patience_cnt >= EARLY_STOP:\n",
    "            print(f'Early stopping triggered at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "# 恢复最佳权重\n",
    "model.load_state_dict(torch.load('best_bilstm.pt', map_location=DEVICE))\n",
    "print(f'\\n最佳验证集 Accuracy: {best_val_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s8",
   "metadata": {},
   "source": [
    "### 8. 训练曲线可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pt-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2, color='steelblue')\n",
    "axes[0].plot(history['val_loss'],   label='Val Loss',   linewidth=2,\n",
    "             linestyle='--', color='tomato')\n",
    "axes[0].set_title('Loss Curve', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train Acc', linewidth=2, color='steelblue')\n",
    "axes[1].plot(history['val_acc'],   label='Val Acc',   linewidth=2,\n",
    "             linestyle='--', color='tomato')\n",
    "axes[1].set_title('Accuracy Curve', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend(); axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('PyTorch BiLSTM — Training History', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_training_curve.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s9",
   "metadata": {},
   "source": [
    "### 9. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pt-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seqs, lengths, labels in val_loader:\n",
    "        seqs    = seqs.to(DEVICE, non_blocking=True)\n",
    "        lengths = lengths.to(DEVICE, non_blocking=True)\n",
    "        logits  = model(seqs, lengths)\n",
    "        preds   = logits.argmax(dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds  = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "sentiment_labels = ['0-Negative', '1-Somewhat Neg', '2-Neutral',\n",
    "                    '3-Somewhat Pos', '4-Positive']\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(all_labels, all_preds, target_names=sentiment_labels))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=sentiment_labels, yticklabels=sentiment_labels)\n",
    "plt.title('Confusion Matrix — PyTorch BiLSTM', fontsize=14)\n",
    "plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lstm_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s10",
   "metadata": {},
   "source": [
    "### 10. 推理示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pt-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(texts_input, model, vocab,\n",
    "                       max_len=MAX_LEN, device=DEVICE):\n",
    "    \"\"\"\n",
    "    对任意文本列表进行情感预测。\n",
    "    返回 DataFrame（text / sentiment / label / confidence）。\n",
    "    \"\"\"\n",
    "    from nltk import word_tokenize\n",
    "\n",
    "    label_map = {0: 'Negative', 1: 'Somewhat Neg', 2: 'Neutral',\n",
    "                 3: 'Somewhat Pos', 4: 'Positive'}\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts_input:\n",
    "            # 与训练时一致的清洗 + 分词\n",
    "            cleaned = text.translate(\n",
    "                str.maketrans('', '', string.punctuation)).lower().strip()\n",
    "            tokens  = word_tokenize(cleaned)[:max_len] or ['<UNK>']\n",
    "\n",
    "            ids     = torch.tensor([vocab.encode(tokens)],\n",
    "                                   dtype=torch.long).to(device)\n",
    "            lengths = torch.tensor([len(tokens)], dtype=torch.long).to(device)\n",
    "            logits  = model(ids, lengths)\n",
    "            probs   = F.softmax(logits, dim=1).squeeze()\n",
    "            pred    = probs.argmax().item()\n",
    "\n",
    "            results.append({\n",
    "                'text'      : text,\n",
    "                'sentiment' : label_map[pred],\n",
    "                'label'     : pred,\n",
    "                'confidence': round(probs[pred].item(), 4)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "demo_texts = [\n",
    "    'This movie is absolutely fantastic and deeply touching',\n",
    "    'Terrible film, complete waste of time',\n",
    "    'It was okay, nothing special',\n",
    "    'A masterpiece of storytelling and raw emotion',\n",
    "    'Boring and utterly predictable plot'\n",
    "]\n",
    "\n",
    "print(predict_sentiment(demo_texts, model, vocab).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pt-s11",
   "metadata": {},
   "source": [
    "### 11. 保存检查点（权重 + 词表 + 超参数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pt-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存完整检查点，之后可直接恢复模型进行推理或继续训练\n",
    "checkpoint = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'vocab'      : vocab,           # Vocabulary 对象（含 word2idx）\n",
    "    'hparams'    : {\n",
    "        'vocab_size' : len(vocab),\n",
    "        'embed_dim'  : EMBED_DIM,\n",
    "        'hidden_dim' : HIDDEN_DIM,\n",
    "        'num_layers' : NUM_LAYERS,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'dropout'    : DROPOUT,\n",
    "        'max_len'    : MAX_LEN,\n",
    "    }\n",
    "}\n",
    "torch.save(checkpoint, 'bilstm_sentiment_checkpoint.pt')\n",
    "print('检查点已保存至 bilstm_sentiment_checkpoint.pt')\n",
    "\n",
    "# ── 加载示例 ──────────────────────────────────────────────────\n",
    "# ckpt         = torch.load('bilstm_sentiment_checkpoint.pt', map_location=DEVICE)\n",
    "# loaded_model = BiLSTMClassifier(**ckpt['hparams']).to(DEVICE)\n",
    "# loaded_model.load_state_dict(ckpt['model_state'])\n",
    "# loaded_vocab = ckpt['vocab']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
